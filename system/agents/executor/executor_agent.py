"""
é«˜åº¦å®Ÿè¡Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

analyzerã®æŠ€è¡“åˆ†æçµæœã‚’å—ã‘ã¦ã€è¤‡æ•°ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§ã®
ä¸¦åˆ—å®Ÿé¨“å®Ÿè¡Œãƒ»GPUæœ€é©åŒ–ãƒ»è‡ªå‹•ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã‚’æ‹…å½“ã™ã‚‹ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
"""

import asyncio
import logging
import uuid
import pandas as pd
from datetime import datetime, timedelta, timezone
from dataclasses import dataclass
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum

# ã‚¯ãƒ©ã‚¦ãƒ‰å®Ÿè¡Œç®¡ç†
from .cloud_managers.kaggle_kernel_manager import KaggleKernelManager
from .cloud_managers.colab_execution_manager import ColabExecutionManager
from .cloud_managers.paperspace_manager import PaperspaceManager
from .cloud_managers.resource_optimizer import CloudResourceOptimizer, ExecutionRequirement

# ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ»å®Ÿé¨“è¨­è¨ˆ
from .code_generators.notebook_generator import NotebookGenerator
from .code_generators.experiment_designer import ExperimentDesigner

# å®Ÿè¡Œã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
from .execution_orchestrator.parallel_executor import ParallelExecutor

# æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³
from .optimization.hyperparameter_tuner import HyperparameterTuner

# GitHub Issueå®‰å…¨ã‚·ã‚¹ãƒ†ãƒ 
from ...issue_safety_system.utils.github_api_wrapper import GitHubApiWrapper
from ...issue_safety_system.concurrency_control.atomic_operations import AtomicIssueOperations

# Kaggle API ã¨ NLP ãƒ¢ãƒ‡ãƒ«
from .kaggle_api_client import KaggleAPIClient

# LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­
from .submission_decision_agent import SubmissionDecisionAgent, SubmissionContext
from ..shared.llm_decision_base import ClaudeClient

class CloudEnvironment(Enum):
    """ã‚¯ãƒ©ã‚¦ãƒ‰å®Ÿè¡Œç’°å¢ƒ"""
    KAGGLE_KERNELS = "kaggle_kernels"
    GOOGLE_COLAB = "google_colab"
    PAPERSPACE_GRADIENT = "paperspace_gradient"

class ExecutionPhase(Enum):
    """å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º"""
    INITIALIZATION = "initialization"
    RESOURCE_PLANNING = "resource_planning"
    CODE_GENERATION = "code_generation"
    PARALLEL_EXECUTION = "parallel_execution"
    OPTIMIZATION = "optimization"
    RESULT_COLLECTION = "result_collection"
    SUBMISSION = "submission"
    COMPLETED = "completed"

class ExecutionPriority(Enum):
    """å®Ÿè¡Œå„ªå…ˆåº¦"""
    MEDAL_CRITICAL = "medal_critical"  # ãƒ¡ãƒ€ãƒ«ç²å¾—é‡è¦
    HIGH = "high"                      # é«˜å„ªå…ˆåº¦
    STANDARD = "standard"              # æ¨™æº–
    EXPERIMENTAL = "experimental"      # å®Ÿé¨“çš„

@dataclass
class ExecutionRequest:
    """å®Ÿè¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    competition_name: str
    analyzer_issue_number: int  # åˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã®Issueç•ªå·
    techniques_to_implement: List[Dict[str, Any]]
    priority: ExecutionPriority
    deadline_days: int
    resource_constraints: Dict[str, Any] = None
    special_requirements: List[str] = None

@dataclass
class ExecutionResult:
    """å®Ÿè¡Œçµæœ"""
    request: ExecutionRequest
    execution_id: str
    
    # å„å®Ÿè¡Œç’°å¢ƒã®çµæœ
    kaggle_results: List[Dict[str, Any]]
    colab_results: List[Dict[str, Any]]
    paperspace_results: List[Dict[str, Any]]
    
    # çµ±åˆçµæœ
    best_score: float
    best_model_config: Dict[str, Any]
    ensemble_performance: Dict[str, Any]
    submission_ready: bool
    
    # å®Ÿè¡Œçµ±è¨ˆ
    total_gpu_hours_used: float
    total_experiments_run: int
    success_rate: float
    resource_efficiency: float
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    execution_duration: float
    phases_completed: List[ExecutionPhase]
    created_at: datetime


class ExecutorAgent:
    """é«˜åº¦å®Ÿè¡Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, github_token: str, repo_name: str):
        self.logger = logging.getLogger(__name__)
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±
        self.agent_id = f"executor-{uuid.uuid4().hex[:8]}"
        self.agent_version = "1.0.0"
        self.start_time = datetime.utcnow()
        
        # GitHub Issueé€£æº
        self.github_wrapper = GitHubApiWrapper(
            access_token=github_token,
            repo_name=repo_name
        )
        self.atomic_operations = AtomicIssueOperations(
            github_client=self.github_wrapper.github,
            repo_name=repo_name
        )
        
        # ã‚¯ãƒ©ã‚¦ãƒ‰å®Ÿè¡Œç®¡ç†
        self.kaggle_manager = KaggleKernelManager()
        self.colab_manager = ColabExecutionManager()
        self.paperspace_manager = PaperspaceManager()
        self.resource_optimizer = CloudResourceOptimizer()
        
        # ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ»å®Ÿé¨“è¨­è¨ˆ
        self.notebook_generator = NotebookGenerator()
        self.experiment_designer = ExperimentDesigner()
        
        # å®Ÿè¡Œã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        self.parallel_executor = ParallelExecutor(
            kaggle_manager=self.kaggle_manager,
            colab_manager=self.colab_manager,
            paperspace_manager=self.paperspace_manager
        )
        
        # æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³
        self.hyperparameter_tuner = HyperparameterTuner()
        
        # Kaggle APIé€£æº
        self.kaggle_client = KaggleAPIClient()
        
        # LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        claude_client = ClaudeClient()
        self.submission_decision_agent = SubmissionDecisionAgent(claude_client)
        
        # å®Ÿè¡Œå±¥æ­´
        self.execution_history: List[ExecutionResult] = []
        self.current_execution: Optional[ExecutionResult] = None
        
        # è¨­å®š
        self.max_concurrent_executions = 3
        self.max_gpu_hours_per_competition = 20
    
    async def execute_technical_implementation(
        self,
        request: ExecutionRequest
    ) -> ExecutionResult:
        """æŠ€è¡“å®Ÿè£…ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        
        execution_start = datetime.utcnow()
        execution_id = f"execution-{uuid.uuid4().hex[:8]}"
        
        self.logger.info(f"æŠ€è¡“å®Ÿè£…å®Ÿè¡Œé–‹å§‹: {request.competition_name} (ID: {execution_id})")
        
        try:
            # å®Ÿè¡Œçµæœã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
            result = ExecutionResult(
                request=request,
                execution_id=execution_id,
                kaggle_results=[],
                colab_results=[],
                paperspace_results=[],
                best_score=0.0,
                best_model_config={},
                ensemble_performance={},
                submission_ready=False,
                total_gpu_hours_used=0.0,
                total_experiments_run=0,
                success_rate=0.0,
                resource_efficiency=0.0,
                execution_duration=0.0,
                phases_completed=[],
                created_at=execution_start
            )
            
            self.current_execution = result
            
            # å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚ºé †æ¬¡å®Ÿè¡Œ
            await self._execute_phase(result, ExecutionPhase.INITIALIZATION)
            await self._execute_phase(result, ExecutionPhase.RESOURCE_PLANNING)
            await self._execute_phase(result, ExecutionPhase.CODE_GENERATION)
            await self._execute_phase(result, ExecutionPhase.PARALLEL_EXECUTION)
            await self._execute_phase(result, ExecutionPhase.OPTIMIZATION)
            await self._execute_phase(result, ExecutionPhase.RESULT_COLLECTION)
            
            # æå‡ºåˆ¤æ–­ãƒ»å®Ÿè¡Œ
            if result.best_score > 0 and result.success_rate > 0.5:
                await self._execute_phase(result, ExecutionPhase.SUBMISSION)
            
            # å®Œäº†å‡¦ç†
            result.execution_duration = (datetime.utcnow() - execution_start).total_seconds()
            result.phases_completed.append(ExecutionPhase.COMPLETED)
            
            # å®Ÿè¡Œå±¥æ­´ã«è¿½åŠ 
            self.execution_history.append(result)
            self.current_execution = None
            
            self.logger.info(f"æŠ€è¡“å®Ÿè£…å®Ÿè¡Œå®Œäº†: {result.execution_duration:.1f}ç§’")
            return result
            
        except Exception as e:
            self.logger.error(f"æŠ€è¡“å®Ÿè£…å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚åŸºæœ¬æƒ…å ±ã¯è¿”ã™
            result.execution_duration = (datetime.utcnow() - execution_start).total_seconds()
            return result
    
    async def _execute_phase(self, result: ExecutionResult, phase: ExecutionPhase):
        """å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚ºå®Ÿè¡Œ"""
        
        phase_start = datetime.utcnow()
        self.logger.info(f"ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹: {phase.value}")
        
        try:
            if phase == ExecutionPhase.INITIALIZATION:
                await self._phase_initialization(result)
            
            elif phase == ExecutionPhase.RESOURCE_PLANNING:
                await self._phase_resource_planning(result)
            
            elif phase == ExecutionPhase.CODE_GENERATION:
                await self._phase_code_generation(result)
            
            elif phase == ExecutionPhase.PARALLEL_EXECUTION:
                await self._phase_parallel_execution(result)
            
            elif phase == ExecutionPhase.OPTIMIZATION:
                await self._phase_optimization(result)
            
            elif phase == ExecutionPhase.RESULT_COLLECTION:
                await self._phase_result_collection(result)
            
            elif phase == ExecutionPhase.SUBMISSION:
                await self._phase_submission(result)
            
            # ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†è¨˜éŒ²
            result.phases_completed.append(phase)
            phase_duration = (datetime.utcnow() - phase_start).total_seconds()
            self.logger.info(f"ãƒ•ã‚§ãƒ¼ã‚ºå®Œäº†: {phase.value} ({phase_duration:.1f}ç§’)")
            
        except Exception as e:
            self.logger.error(f"ãƒ•ã‚§ãƒ¼ã‚ºã‚¨ãƒ©ãƒ¼: {phase.value} - {e}")
    
    async def _phase_initialization(self, result: ExecutionResult):
        """åˆæœŸåŒ–ãƒ•ã‚§ãƒ¼ã‚º"""
        
        request = result.request
        
        # analyzerã‹ã‚‰ã®æŠ€è¡“æŒ‡é‡ã‚’å–å¾—
        analyzer_issue = await self.github_wrapper.get_issue_safely(
            request.analyzer_issue_number
        )
        
        if analyzer_issue.success:
            # Issueæœ¬æ–‡ã‹ã‚‰æŠ€è¡“æ¨å¥¨ã‚’è§£æ
            result.analyzer_recommendations = await self._parse_analyzer_recommendations(
                analyzer_issue.issue.body
            )
        
        # ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã®æ­£è¦åŒ–
        result.resource_constraints = {
            "max_gpu_hours": min(request.deadline_days * 2, self.max_gpu_hours_per_competition),
            "preferred_environments": self._determine_preferred_environments(request.priority),
            "parallel_limit": 3 if request.priority == ExecutionPriority.MEDAL_CRITICAL else 2
        }
        
        self.logger.info(f"åˆæœŸåŒ–å®Œäº†: {request.competition_name}")
    
    async def _phase_resource_planning(self, result: ExecutionResult):
        """ãƒªã‚½ãƒ¼ã‚¹è¨ˆç”»ãƒ•ã‚§ãƒ¼ã‚º"""
        
        # æŠ€è¡“å®Ÿè£…ã®è¨ˆç®—ã‚³ã‚¹ãƒˆæ¨å®š
        implementation_costs = []
        for technique in result.request.techniques_to_implement:
            cost = await self.resource_optimizer.estimate_implementation_cost(
                technique_name=technique["technique"],
                complexity=technique.get("integrated_score", 0.5),
                gpu_requirement=technique.get("gpu_required", False)
            )
            implementation_costs.append(cost)
        
        # ExecutionRequirementã«å¤‰æ›
        execution_requirements = []
        for i, technique in enumerate(result.request.techniques_to_implement):
            req = ExecutionRequirement(
                technique_name=technique.get("technique", f"technique_{i}"),
                competition_name=result.request.competition_name,
                estimated_gpu_hours=implementation_costs[i].get("gpu_hours", 2.0),
                estimated_cpu_hours=implementation_costs[i].get("cpu_hours", 1.0),
                memory_gb_required=4.0,
                storage_gb_required=2.0,
                deadline_hours=result.request.deadline_days * 24,
                priority_score=technique.get("integrated_score", 0.5),
                complexity_level=0.5
            )
            execution_requirements.append(req)
        
        # æœ€é©ãªãƒªã‚½ãƒ¼ã‚¹é…åˆ†ã‚’æ±ºå®š
        allocations = await self.resource_optimizer.optimize_experiment_allocation(
            experiments=execution_requirements
        )
        
        # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        result.resource_allocation = {}
        for allocation in allocations:
            result.resource_allocation[allocation.technique_name] = {
                "environments": [allocation.platform.value],
                "gpu_hours": allocation.allocated_gpu_hours
            }
        
        self.logger.info(f"ãƒªã‚½ãƒ¼ã‚¹è¨ˆç”»å®Œäº†: {len(result.resource_allocation)}æŠ€è¡“é…åˆ†")
    
    async def _phase_code_generation(self, result: ExecutionResult):
        """ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º"""
        
        # å„æŠ€è¡“ã®å®Ÿè£…ç”¨ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç”Ÿæˆ
        result.generated_notebooks = {}
        
        for technique in result.request.techniques_to_implement:
            # ç’°å¢ƒåˆ¥æœ€é©åŒ–ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç”Ÿæˆ
            notebooks = await self.notebook_generator.generate_technique_notebooks(
                technique_name=technique["technique"],
                competition_type=result.request.competition_name.split('_')[0],  # ç°¡æ˜“ã‚¿ã‚¤ãƒ—æ¨å®š
                target_environments=result.resource_allocation.get(technique["technique"], {}).get("environments", ["kaggle"])
            )
            
            result.generated_notebooks[technique["technique"]] = notebooks
        
        # å®Ÿé¨“è¨­è¨ˆãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç©ºé–“å®šç¾©
        result.experiment_designs = await self.experiment_designer.design_experiments(
            techniques=result.request.techniques_to_implement,
            notebooks=result.generated_notebooks,
            resource_constraints=result.resource_constraints
        )
        
        self.logger.info(f"ã‚³ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†: {len(result.generated_notebooks)}æŠ€è¡“ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯")
    
    async def _phase_parallel_execution(self, result: ExecutionResult):
        """ä¸¦åˆ—å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º"""
        
        # è¤‡æ•°ç’°å¢ƒã§ã®ä¸¦åˆ—å®Ÿé¨“å®Ÿè¡Œ
        execution_tasks = []
        
        for technique, allocation in result.resource_allocation.items():
            for env in allocation.get("environments", ["kaggle"]):
                task = self.parallel_executor.execute_technique(
                    technique_name=technique,
                    notebook=result.generated_notebooks[technique][env],
                    environment=CloudEnvironment(env),
                    resource_limit=allocation.get("gpu_hours", 2.0)
                )
                execution_tasks.append(task)
        
        # ä¸¦åˆ—å®Ÿè¡Œï¼ˆä¾‹å¤–ã¯å€‹åˆ¥ã«å‡¦ç†ï¼‰
        execution_results = await asyncio.gather(*execution_tasks, return_exceptions=True)
        
        # çµæœã‚’ç’°å¢ƒåˆ¥ã«åˆ†é¡
        for i, exec_result in enumerate(execution_results):
            if isinstance(exec_result, Exception):
                self.logger.error(f"å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ {i}: {exec_result}")
                continue
            
            env = exec_result.get("environment", "unknown")
            if env == "kaggle_kernels":
                result.kaggle_results.append(exec_result)
            elif env == "google_colab":
                result.colab_results.append(exec_result)
            elif env == "paperspace_gradient":
                result.paperspace_results.append(exec_result)
        
        # å®Ÿè¡Œçµ±è¨ˆæ›´æ–°
        all_results = result.kaggle_results + result.colab_results + result.paperspace_results
        result.total_experiments_run = len(all_results)
        result.success_rate = len([r for r in all_results if r.get("success", False)]) / max(len(all_results), 1)
        result.total_gpu_hours_used = sum(r.get("gpu_hours_used", 0) for r in all_results)
        
        self.logger.info(f"ä¸¦åˆ—å®Ÿè¡Œå®Œäº†: {result.total_experiments_run}å®Ÿé¨“, æˆåŠŸç‡{result.success_rate:.1%}")
    
    async def _phase_optimization(self, result: ExecutionResult):
        """æœ€é©åŒ–ãƒ•ã‚§ãƒ¼ã‚º"""
        
        if not result.kaggle_results and not result.colab_results and not result.paperspace_results:
            self.logger.warning("æœ€é©åŒ–å¯¾è±¡ã®çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ç‰¹å®š
        all_results = result.kaggle_results + result.colab_results + result.paperspace_results
        successful_results = [r for r in all_results if r.get("success", False)]
        
        if successful_results:
            best_result = max(successful_results, key=lambda x: x.get("score", 0))
            result.best_score = best_result.get("score", 0)
            result.best_model_config = best_result.get("model_config", {})
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ€§èƒ½è©•ä¾¡
        if len(successful_results) >= 2:
            result.ensemble_performance = await self.model_selection.evaluate_ensemble_potential(
                successful_results
            )
        
        # ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ç®—å‡º
        if result.total_gpu_hours_used > 0:
            result.resource_efficiency = result.best_score / result.total_gpu_hours_used
        
        self.logger.info(f"æœ€é©åŒ–å®Œäº†: æœ€é«˜ã‚¹ã‚³ã‚¢{result.best_score:.4f}")
    
    async def _phase_result_collection(self, result: ExecutionResult):
        """çµæœåé›†ãƒ•ã‚§ãƒ¼ã‚º"""
        
        # çµæœã®çµ±åˆãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        result.execution_summary = {
            "total_techniques_attempted": len(result.request.techniques_to_implement),
            "successful_implementations": len([r for r in (result.kaggle_results + result.colab_results + result.paperspace_results) if r.get("success", False)]),
            "resource_utilization": {
                "gpu_hours_used": result.total_gpu_hours_used,
                "gpu_hours_allocated": result.resource_constraints["max_gpu_hours"],
                "utilization_rate": result.total_gpu_hours_used / result.resource_constraints["max_gpu_hours"]
            },
            "performance_metrics": {
                "best_score": result.best_score,
                "success_rate": result.success_rate,
                "resource_efficiency": result.resource_efficiency
            }
        }
        
        # æå‡ºæº–å‚™åˆ¤å®š
        result.submission_ready = (
            result.best_score > 0 and
            result.success_rate > 0.3 and
            len(result.phases_completed) >= 5
        )
        
        self.logger.info(f"çµæœåé›†å®Œäº†: æå‡ºæº–å‚™{'å®Œäº†' if result.submission_ready else 'æœªå®Œ'}")
    
    async def _phase_submission(self, result: ExecutionResult):
        """æå‡ºãƒ•ã‚§ãƒ¼ã‚º - LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­çµ±åˆ"""
        
        if not result.submission_ready:
            self.logger.warning("æå‡ºæº–å‚™æœªå®Œã®ãŸã‚æå‡ºãƒ•ã‚§ãƒ¼ã‚ºã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return
        
        try:
            # LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­å®Ÿè¡Œ
            submission_context = await self._create_submission_context(result)
            
            # ç·Šæ€¥åº¦åˆ¤å®š
            urgency = self._determine_submission_urgency(result)
            
            # LLMæå‡ºåˆ¤æ–­
            decision_response = await self.submission_decision_agent.should_submit_competition(
                context=submission_context,
                urgency=urgency
            )
            
            self.logger.info(
                f"ğŸ¤– LLMæå‡ºåˆ¤æ–­: {decision_response.decision_result['decision']} "
                f"(ä¿¡é ¼åº¦: {decision_response.confidence_score:.2f})"
            )
            
            # åˆ¤æ–­ã«åŸºã¥ãå®Ÿè¡Œ
            decision = decision_response.decision_result["decision"]
            
            if decision == "SUBMIT":
                # æå‡ºå®Ÿè¡Œ
                competition_name = result.request.competition_name.lower().replace(' ', '-')
                await self._submit_generic_competition(result, competition_name)
                
                # LLMåˆ¤æ–­å±¥æ­´è¨˜éŒ²
                result.llm_submission_decision = decision_response.decision_result
                
            elif decision == "CONTINUE":
                # å®Ÿé¨“ç¶™ç¶šæŒ‡ç¤º
                self.logger.info("ğŸ”„ LLMåˆ¤æ–­: å®Ÿé¨“ç¶™ç¶šæ¨å¥¨")
                result.submission_info = {
                    "submitted": False, 
                    "reason": "llm_decision_continue",
                    "llm_reasoning": decision_response.reasoning
                }
                
                # ç¶™ç¶šå®Ÿé¨“ã®å®Ÿè¡Œï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
                await self._execute_continued_experiments(result, decision_response)
                
            else:  # "WAIT"
                # å¾…æ©Ÿ
                self.logger.info("â³ LLMåˆ¤æ–­: å¾…æ©Ÿæ¨å¥¨")
                result.submission_info = {
                    "submitted": False,
                    "reason": "llm_decision_wait", 
                    "llm_reasoning": decision_response.reasoning,
                    "next_evaluation_time": decision_response.decision_result.get("timeline_recommendation", "1æ™‚é–“å¾Œ")
                }
                
        except Exception as e:
            self.logger.error(f"LLMæå‡ºåˆ¤æ–­ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æå‡ºåˆ¤æ–­
            if result.best_score > 0 and result.success_rate > 0.5:
                competition_name = result.request.competition_name.lower().replace(' ', '-')
                await self._submit_generic_competition(result, competition_name)
            
            result.submission_info = {"submitted": False, "error": str(e), "fallback_used": True}
    
    async def _create_submission_context(self, result: ExecutionResult) -> SubmissionContext:
        """æå‡ºåˆ¤æ–­ç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ"""
        
        request = result.request
        
        # ã‚¹ã‚³ã‚¢å±¥æ­´ï¼ˆæ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ï¼‰
        score_history = [result.best_score * (0.8 + i * 0.05) for i in range(5)]
        if len(score_history) > 1:
            recent_improvement = score_history[-1] - score_history[-2] 
        else:
            recent_improvement = 0.0
        
        # ç«¶åˆæƒ…å ±ï¼ˆæ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ï¼‰
        leaderboard_top10 = [result.best_score * (1.1 + i * 0.02) for i in range(10)]
        medal_threshold = result.best_score * 0.95
        
        # ãƒ¡ãƒ€ãƒ«åœåˆ¤å®š
        if result.best_score >= leaderboard_top10[2]:
            medal_zone = "gold"
        elif result.best_score >= leaderboard_top10[5]:
            medal_zone = "silver"
        elif result.best_score >= medal_threshold:
            medal_zone = "bronze"
        else:
            medal_zone = "none"
        
        # ç· åˆ‡ã¾ã§ã®æ™‚é–“ï¼ˆæ¨¡æ“¬ï¼‰
        deadline_hours = max(6, request.deadline_days * 24 - 24)  # 1æ—¥å‰ã‚’ä»®å®š
        
        return SubmissionContext(
            competition_name=request.competition_name,
            current_best_score=result.best_score,
            target_score=result.best_score * 1.1,  # 10%æ”¹å–„ç›®æ¨™
            current_rank_estimate=max(50, 1000 - int(result.best_score * 1000)),
            total_participants=5000,  # ä»®æƒ³å‚åŠ è€…æ•°
            days_remaining=max(1, request.deadline_days - 1),
            hours_remaining=deadline_hours,
            
            experiments_completed=result.total_experiments_run,
            experiments_running=0,  # ç¾åœ¨ã¯å®Ÿè¡Œä¸­å®Ÿé¨“ã‚’è¿½è·¡ã—ã¦ã„ãªã„
            success_rate=result.success_rate,
            resource_budget_remaining=max(0.1, 1.0 - (result.total_gpu_hours_used / 20.0)),
            
            score_history=score_history,
            score_improvement_trend=recent_improvement,
            plateau_duration_hours=max(0, 12 - recent_improvement * 100),
            
            leaderboard_top10_scores=leaderboard_top10,
            medal_threshold_estimate=medal_threshold,
            current_medal_zone=medal_zone,
            
            model_stability=min(1.0, result.success_rate + 0.2),
            overfitting_risk=max(0.0, 0.8 - result.success_rate),
            technical_debt_level=0.3  # å›ºå®šå€¤
        )
    
    def _determine_submission_urgency(self, result: ExecutionResult) -> str:
        """æå‡ºç·Šæ€¥åº¦åˆ¤å®š"""
        
        hours_remaining = max(6, result.request.deadline_days * 24 - 24)
        
        if hours_remaining < 12:
            return "critical"
        elif hours_remaining < 48:
            return "high"
        elif hours_remaining < 120:  # 5æ—¥
            return "medium"
        else:
            return "low"
    
    async def _execute_continued_experiments(
        self, 
        result: ExecutionResult, 
        decision_response
    ):
        """ç¶™ç¶šå®Ÿé¨“å®Ÿè¡Œï¼ˆLLMæ¨å¥¨ã«åŸºã¥ãï¼‰"""
        
        try:
            alternative_actions = decision_response.decision_result.get("alternative_actions", [])
            
            self.logger.info(f"ğŸ”¬ ç¶™ç¶šå®Ÿé¨“é–‹å§‹: {len(alternative_actions)}ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
            
            for action in alternative_actions[:2]:  # æœ€å¤§2ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
                if "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿" in action:
                    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å®Ÿé¨“
                    self.logger.info(f"ğŸ›ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å®Ÿè¡Œ: {action}")
                    
                elif "ç‰¹å¾´é‡" in action:
                    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
                    self.logger.info(f"ğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°: {action}")
                    
                elif "ãƒ¢ãƒ‡ãƒ«" in action:
                    # ãƒ¢ãƒ‡ãƒ«å¤‰æ›´å®Ÿé¨“
                    self.logger.info(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«å®Ÿé¨“: {action}")
                
                # å®Ÿéš›ã®å®Ÿé¨“å®Ÿè¡Œï¼ˆæ¨¡æ“¬ï¼‰
                await asyncio.sleep(1)  # å®Ÿé¨“æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            
            # ç¶™ç¶šå®Ÿé¨“å®Œäº†ã‚’è¨˜éŒ²
            result.continued_experiments = {
                "executed_actions": alternative_actions[:2],
                "execution_time": datetime.utcnow(),
                "triggered_by": "llm_submission_decision"
            }
            
        except Exception as e:
            self.logger.error(f"ç¶™ç¶šå®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _submit_generic_competition(self, result: ExecutionResult, competition_name: str):
        """æ±ç”¨ç«¶æŠ€æå‡ºå‡¦ç†"""
        
        self.logger.info(f"ğŸš€ ç«¶æŠ€æå‡ºé–‹å§‹: {competition_name}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            dataset = await self.kaggle_client.download_competition_data(competition_name)
            if not dataset:
                raise ValueError("ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—")
            
            # åŸºæœ¬çš„ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆæ±ç”¨ï¼‰
            from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
            from sklearn.preprocessing import LabelEncoder
            import numpy as np
            
            # ç‰¹å¾´é‡ãƒ»ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè‡ªå‹•æ¤œå‡º
            train_data = dataset.train_data.copy()
            test_data = dataset.test_data.copy()
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’ç‰¹å®šï¼ˆä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            target_col = None
            for col in ['target', 'Survived', 'SalePrice', 'label', 'y']:
                if col in train_data.columns:
                    target_col = col
                    break
            
            if not target_col:
                raise ValueError("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ãŒç‰¹å®šã§ãã¾ã›ã‚“")
            
            # æ•°å€¤ç‰¹å¾´é‡ã‚’è‡ªå‹•é¸æŠ
            numeric_features = train_data.select_dtypes(include=[np.number]).columns.tolist()
            if target_col in numeric_features:
                numeric_features.remove(target_col)
            if 'Id' in numeric_features:
                numeric_features.remove('Id')
            
            # åŸºæœ¬çš„ãªå‰å‡¦ç†
            for col in numeric_features:
                if col in train_data.columns and col in test_data.columns:
                    train_data[col].fillna(train_data[col].median(), inplace=True)
                    test_data[col].fillna(train_data[col].median(), inplace=True)
            
            # ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã‚‚å‡¦ç†
            categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()
            le_dict = {}
            
            for col in categorical_features:
                if col in test_data.columns:
                    le = LabelEncoder()
                    combined = pd.concat([train_data[col].astype(str), test_data[col].astype(str)])
                    le.fit(combined)
                    train_data[col] = le.transform(train_data[col].astype(str))
                    test_data[col] = le.transform(test_data[col].astype(str))
                    le_dict[col] = le
            
            # ç‰¹å¾´é‡çµåˆ
            all_features = numeric_features + categorical_features
            available_features = [f for f in all_features if f in train_data.columns and f in test_data.columns]
            
            # ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆåˆ†é¡ vs å›å¸°ï¼‰
            target_values = train_data[target_col]
            is_classification = len(target_values.unique()) < 20 and target_values.dtype in ['int64', 'bool']
            
            X_train = train_data[available_features]
            y_train = train_data[target_col]
            X_test = test_data[available_features]
            
            if is_classification:
                model = RandomForestClassifier(n_estimators=100, random_state=42)
                model_type = "Classification"
            else:
                model = RandomForestRegressor(n_estimators=100, random_state=42)
                model_type = "Regression"
            
            model.fit(X_train, y_train)
            
            # äºˆæ¸¬
            predictions = model.predict(X_test)
            
            # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            submission_df = dataset.sample_submission.copy()
            submission_df.iloc[:, 1] = predictions  # 2åˆ—ç›®ã«äºˆæ¸¬å€¤
            
            # Kaggleæå‡º
            submission_result = await self.kaggle_client.submit_predictions(
                competition_name=competition_name,
                predictions_df=submission_df,
                description=f"Claude {model_type} RandomForest Baseline"
            )
            
            if submission_result:
                result.submission_info = {
                    "submitted": True,
                    "submission_id": submission_result.submission_id,
                    "model_type": f"RandomForest{model_type}",
                    "features_used": len(available_features),
                    "submission_timestamp": datetime.now(timezone.utc)
                }
                
                self.logger.info(f"âœ… ç«¶æŠ€æå‡ºå®Œäº†: {submission_result.submission_id}")
            else:
                raise ValueError("Kaggleæå‡ºå¤±æ•—")
                
        except Exception as e:
            self.logger.error(f"ç«¶æŠ€æå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            result.submission_info = {"submitted": False, "error": str(e)}
    
    async def _parse_analyzer_recommendations(self, issue_body: str) -> Dict[str, Any]:
        """analyzerã‹ã‚‰ã®æŠ€è¡“æ¨å¥¨è§£æ"""
        
        # ç°¡æ˜“çš„ãªæ¨å¥¨æŠ€è¡“è§£æï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šé«˜åº¦ãªãƒ‘ãƒ¼ã‚¹å‡¦ç†ï¼‰
        recommendations = {
            "priority_techniques": [],
            "implementation_timeline": [],
            "resource_requirements": {},
            "risk_factors": []
        }
        
        # Issueæœ¬æ–‡ã‹ã‚‰æŠ€è¡“åã‚’æŠ½å‡ºï¼ˆä¾‹ï¼‰
        if "gradient_boosting" in issue_body.lower():
            recommendations["priority_techniques"].append("gradient_boosting_ensemble")
        if "stacking" in issue_body.lower():
            recommendations["priority_techniques"].append("multi_level_stacking")
        
        return recommendations
    
    def _determine_preferred_environments(self, priority: ExecutionPriority) -> List[str]:
        """å„ªå…ˆåº¦ã«åŸºã¥ãæ¨å¥¨ç’°å¢ƒæ±ºå®š"""
        
        if priority == ExecutionPriority.MEDAL_CRITICAL:
            return ["kaggle_kernels", "google_colab", "paperspace_gradient"]
        elif priority == ExecutionPriority.HIGH:
            return ["kaggle_kernels", "google_colab"]
        else:
            return ["kaggle_kernels"]
    
    async def get_agent_status(self) -> Dict[str, Any]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçŠ¶æ…‹å–å¾—"""
        
        uptime = (datetime.utcnow() - self.start_time).total_seconds() / 3600
        
        return {
            "agent_id": self.agent_id,
            "agent_version": self.agent_version,
            "uptime_hours": uptime,
            "execution_history_count": len(self.execution_history),
            "current_execution_active": self.current_execution is not None,
            "resource_status": await self.resource_optimizer.get_current_quotas(),
            "last_execution": {
                "competition_name": self.execution_history[-1].request.competition_name if self.execution_history else None,
                "best_score": self.execution_history[-1].best_score if self.execution_history else None,
                "success_rate": self.execution_history[-1].success_rate if self.execution_history else None
            } if self.execution_history else None
        }
    
    async def execute_quick_test(
        self,
        competition_name: str = "test_competition",
        techniques: List[str] = None
    ) -> Dict[str, Any]:
        """ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆé–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        
        techniques = techniques or ["gradient_boosting_ensemble"]
        
        quick_request = ExecutionRequest(
            competition_name=competition_name,
            analyzer_issue_number=12345,  # ãƒ†ã‚¹ãƒˆç”¨
            techniques_to_implement=[
                {"technique": tech, "integrated_score": 0.8, "gpu_required": False}
                for tech in techniques
            ],
            priority=ExecutionPriority.EXPERIMENTAL,
            deadline_days=1
        )
        
        result = await self.execute_technical_implementation(quick_request)
        
        return {
            "execution_id": result.execution_id,
            "success_rate": result.success_rate,
            "total_experiments": result.total_experiments_run,
            "gpu_hours_used": result.total_gpu_hours_used,
            "execution_duration": result.execution_duration,
            "submission_ready": result.submission_ready
        }