"""
é«˜åº¦åçœã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

Kaggleç«¶æŠ€çµ‚äº†å¾Œã®å…¨æ´»å‹•æŒ¯ã‚Šè¿”ã‚Šãƒ»æˆåŠŸå¤±æ•—åˆ†æãƒ»æ”¹å–„ç‚¹ç‰¹å®šãƒ»
çŸ¥è­˜è“„ç©ã«ã‚ˆã‚‹ã‚·ã‚¹ãƒ†ãƒ ç¶™ç¶šå­¦ç¿’ã‚’å®Ÿç¾ã™ã‚‹ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
"""

import asyncio
import logging
import json
import uuid
import statistics
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
import re

# GitHub Issueå®‰å…¨ã‚·ã‚¹ãƒ†ãƒ 
from ...issue_safety_system.utils.github_api_wrapper import GitHubApiWrapper
from ...issue_safety_system.concurrency_control.atomic_operations import AtomicIssueOperations

# ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®é€£æº
from ..analyzer.analyzer_agent import AnalyzerAgent
from ..executor.executor_agent import ExecutorAgent
from ..monitor.monitor_agent import MonitorAgent


class RetrospectiveDepth(Enum):
    """æŒ¯ã‚Šè¿”ã‚Šæ·±åº¦"""
    SURFACE = "surface"      # è¡¨é¢çš„ï¼šåŸºæœ¬çµ±è¨ˆã®ã¿
    STANDARD = "standard"    # æ¨™æº–ï¼šä¸»è¦ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    DEEP = "deep"           # æ·±å±¤ï¼šè©³ç´°å› å­åˆ†æ
    COMPREHENSIVE = "comprehensive"  # åŒ…æ‹¬çš„ï¼šå…¨é¢çš„åˆ†æ


class LearningCategory(Enum):
    """å­¦ç¿’ã‚«ãƒ†ã‚´ãƒª"""
    TECHNIQUE_EFFECTIVENESS = "technique_effectiveness"  # æŠ€è¡“æœ‰åŠ¹æ€§
    RESOURCE_OPTIMIZATION = "resource_optimization"     # ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–
    TIME_MANAGEMENT = "time_management"                  # æ™‚é–“ç®¡ç†
    ERROR_PATTERNS = "error_patterns"                    # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
    SUCCESS_FACTORS = "success_factors"                  # æˆåŠŸè¦å› 
    COMPETITION_PATTERNS = "competition_patterns"        # ç«¶æŠ€ãƒ‘ã‚¿ãƒ¼ãƒ³


class ImprovementPriority(Enum):
    """æ”¹å–„å„ªå…ˆåº¦"""
    CRITICAL = "critical"    # é‡è¦ï¼šæ¬¡å›å¿…é ˆæ”¹å–„
    HIGH = "high"           # é«˜ï¼šæ—©æœŸæ”¹å–„æ¨å¥¨
    MEDIUM = "medium"       # ä¸­ï¼šä¸­æœŸæ”¹å–„æ¤œè¨
    LOW = "low"            # ä½ï¼šé•·æœŸæ”¹å–„å€™è£œ


@dataclass
class CompetitionSummary:
    """ç«¶æŠ€ã‚µãƒãƒªãƒ¼"""
    competition_name: str
    competition_type: str
    start_date: datetime
    end_date: datetime
    deadline: datetime
    
    # å‚åŠ çŠ¶æ³
    participants_count: int
    final_ranking: Optional[int] = None
    final_percentile: Optional[float] = None
    medal_achieved: Optional[str] = None  # gold, silver, bronze, None
    
    # ã‚¹ã‚³ã‚¢æ¨ç§»
    initial_score: float = 0.0
    best_score: float = 0.0
    final_score: float = 0.0
    score_improvement: float = 0.0
    
    # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨
    total_gpu_hours_used: float = 0.0
    total_api_calls: int = 0
    total_experiments: int = 0
    
    # å®Ÿè£…çŠ¶æ³
    techniques_identified: int = 0
    techniques_attempted: int = 0
    techniques_successful: int = 0


@dataclass
class TechniqueAnalysis:
    """æŠ€è¡“åˆ†æçµæœ"""
    technique_name: str
    category: str
    
    # å®Ÿè¡Œçµ±è¨ˆ
    attempts_count: int
    success_count: int
    failure_count: int
    success_rate: float
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    avg_score_improvement: float
    best_score_achieved: float
    avg_execution_time_hours: float
    total_gpu_hours_used: float
    
    # æœ‰åŠ¹æ€§è©•ä¾¡
    effectiveness_score: float  # 0.0-1.0
    difficulty_score: float     # 0.0-1.0
    roi_score: float           # return on investment
    
    # å¤±æ•—è¦å› 
    common_failures: List[str] = field(default_factory=list)
    implementation_challenges: List[str] = field(default_factory=list)
    
    # æˆåŠŸè¦å› 
    success_factors: List[str] = field(default_factory=list)
    optimal_conditions: List[str] = field(default_factory=list)


@dataclass
class LearningInsight:
    """å­¦ç¿’çŸ¥è¦‹"""
    insight_id: str
    category: LearningCategory
    title: str
    description: str
    
    # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹
    supporting_data: Dict[str, Any]
    confidence_level: float  # 0.0-1.0
    statistical_significance: float
    
    # é©ç”¨ç¯„å›²
    applicable_competition_types: List[str]
    applicable_contexts: List[str]
    
    # æ”¹å–„ææ¡ˆ
    improvement_recommendations: List[str]
    priority: ImprovementPriority
    
    # ãƒ¡ã‚¿æƒ…å ±
    discovered_at: datetime
    validated: bool = False
    applied_count: int = 0


@dataclass
class RetrospectiveReport:
    """æŒ¯ã‚Šè¿”ã‚Šãƒ¬ãƒãƒ¼ãƒˆ"""
    report_id: str
    competition_summary: CompetitionSummary
    generated_at: datetime
    analysis_depth: RetrospectiveDepth
    
    # å…¨ä½“è©•ä¾¡
    overall_performance: str  # excellent, good, fair, poor
    key_achievements: List[str]
    major_failures: List[str]
    
    # æŠ€è¡“åˆ†æ
    technique_analyses: List[TechniqueAnalysis]
    
    # å­¦ç¿’çŸ¥è¦‹
    new_insights: List[LearningInsight]
    validated_insights: List[LearningInsight]
    
    # ãƒªã‚½ãƒ¼ã‚¹åˆ†æ
    resource_efficiency: Dict[str, float]
    time_allocation_analysis: Dict[str, float]
    
    # æ”¹å–„ææ¡ˆ
    immediate_improvements: List[str]
    strategic_improvements: List[str]
    
    # æ¬¡å›é©ç”¨äº‹é …
    lessons_for_next_competition: List[str]
    updated_best_practices: List[str]


class RetrospectiveAgent:
    """é«˜åº¦åçœã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, github_token: str, repo_name: str):
        self.logger = logging.getLogger(__name__)
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±
        self.agent_id = f"retrospective-{uuid.uuid4().hex[:8]}"
        self.agent_version = "1.0.0"
        self.start_time = datetime.utcnow()
        
        # GitHub Issueé€£æº
        self.github_wrapper = GitHubApiWrapper(
            access_token=github_token,
            repo_name=repo_name
        )
        self.atomic_operations = AtomicIssueOperations(
            github_client=self.github_wrapper.github,
            repo_name=repo_name
        )
        
        # çŸ¥è­˜è“„ç©ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.historical_insights: List[LearningInsight] = []
        self.technique_knowledge: Dict[str, TechniqueAnalysis] = {}
        self.competition_patterns: Dict[str, Any] = {}
        
        # åˆ†æè¨­å®š
        self.default_analysis_depth = RetrospectiveDepth.STANDARD
        self.confidence_threshold = 0.7
        self.statistical_significance_threshold = 0.05
        
        # æŒ¯ã‚Šè¿”ã‚Šå±¥æ­´
        self.retrospective_history: List[RetrospectiveReport] = []
    
    async def conduct_competition_retrospective(
        self,
        competition_data: Dict[str, Any],
        agent_histories: Dict[str, List[Any]],
        analysis_depth: RetrospectiveDepth = None
    ) -> RetrospectiveReport:
        """ç«¶æŠ€æŒ¯ã‚Šè¿”ã‚Šå®Ÿè¡Œ"""
        
        analysis_depth = analysis_depth or self.default_analysis_depth
        
        self.logger.info(f"ç«¶æŠ€æŒ¯ã‚Šè¿”ã‚Šé–‹å§‹: {competition_data.get('name', 'Unknown')} (æ·±åº¦: {analysis_depth.value})")
        
        # GitHub Issueä½œæˆ: æŒ¯ã‚Šè¿”ã‚Šé–‹å§‹é€šçŸ¥
        retrospective_issue = await self._create_retrospective_issue(
            competition_data, analysis_depth
        )
        
        try:
            # 1. ç«¶æŠ€ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            competition_summary = await self._generate_competition_summary(
                competition_data, agent_histories
            )
            
            # 2. æŠ€è¡“åˆ†æå®Ÿè¡Œ
            technique_analyses = await self._analyze_techniques(
                agent_histories.get("executor", []), analysis_depth
            )
            
            # 3. å­¦ç¿’çŸ¥è¦‹æŠ½å‡º
            new_insights = await self._extract_learning_insights(
                competition_summary, technique_analyses, agent_histories, analysis_depth
            )
            
            # 4. æ—¢å­˜çŸ¥è¦‹æ¤œè¨¼
            validated_insights = await self._validate_existing_insights(
                competition_summary, technique_analyses
            )
            
            # 5. ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡æ€§åˆ†æ
            resource_analysis = await self._analyze_resource_efficiency(
                agent_histories, competition_summary
            )
            
            # 6. æ”¹å–„ææ¡ˆç”Ÿæˆ
            improvements = await self._generate_improvement_recommendations(
                competition_summary, technique_analyses, new_insights
            )
            
            # 7. æŒ¯ã‚Šè¿”ã‚Šãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            report = RetrospectiveReport(
                report_id=f"retro-{uuid.uuid4().hex[:8]}",
                competition_summary=competition_summary,
                generated_at=datetime.utcnow(),
                analysis_depth=analysis_depth,
                overall_performance=self._evaluate_overall_performance(competition_summary),
                key_achievements=improvements["achievements"],
                major_failures=improvements["failures"],
                technique_analyses=technique_analyses,
                new_insights=new_insights,
                validated_insights=validated_insights,
                resource_efficiency=resource_analysis["efficiency"],
                time_allocation_analysis=resource_analysis["time_allocation"],
                immediate_improvements=improvements["immediate"],
                strategic_improvements=improvements["strategic"],
                lessons_for_next_competition=improvements["lessons"],
                updated_best_practices=improvements["best_practices"]
            )
            
            # 8. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°
            await self._update_knowledge_base(report)
            
            # 9. ãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿
            await self._post_retrospective_report(report, retrospective_issue)
            
            # 10. å±¥æ­´ã«è¿½åŠ 
            self.retrospective_history.append(report)
            
            self.logger.info(f"ç«¶æŠ€æŒ¯ã‚Šè¿”ã‚Šå®Œäº†: {report.report_id}")
            return report
            
        except Exception as e:
            self.logger.error(f"ç«¶æŠ€æŒ¯ã‚Šè¿”ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
            await self._post_error_report(e, retrospective_issue)
            raise
    
    async def _generate_competition_summary(
        self,
        competition_data: Dict[str, Any],
        agent_histories: Dict[str, List[Any]]
    ) -> CompetitionSummary:
        """ç«¶æŠ€ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        
        # åŸºæœ¬æƒ…å ±æŠ½å‡º
        name = competition_data.get("name", "Unknown Competition")
        comp_type = competition_data.get("type", "tabular")
        start_date = datetime.fromisoformat(competition_data.get("start_date", datetime.utcnow().isoformat()))
        end_date = datetime.fromisoformat(competition_data.get("end_date", datetime.utcnow().isoformat()))
        deadline = datetime.fromisoformat(competition_data.get("deadline", datetime.utcnow().isoformat()))
        
        # å®Ÿè¡Œå±¥æ­´ã‹ã‚‰çµ±è¨ˆæŠ½å‡º
        executor_history = agent_histories.get("executor", [])
        analyzer_history = agent_histories.get("analyzer", [])
        
        # ã‚¹ã‚³ã‚¢çµ±è¨ˆ
        all_scores = []
        total_gpu_hours = 0.0
        total_experiments = 0
        
        for execution in executor_history:
            if hasattr(execution, 'best_score') and execution.best_score > 0:
                all_scores.append(execution.best_score)
            if hasattr(execution, 'total_gpu_hours_used'):
                total_gpu_hours += execution.total_gpu_hours_used
            if hasattr(execution, 'total_experiments_run'):
                total_experiments += execution.total_experiments_run
        
        # æŠ€è¡“çµ±è¨ˆ
        techniques_identified = 0
        techniques_attempted = 0
        techniques_successful = 0
        
        for analysis in analyzer_history:
            if hasattr(analysis, 'recommended_techniques'):
                techniques_identified += len(analysis.recommended_techniques)
        
        for execution in executor_history:
            if hasattr(execution, 'techniques_to_implement'):
                techniques_attempted += len(execution.techniques_to_implement)
            if hasattr(execution, 'success_rate') and execution.success_rate > 0.5:
                techniques_successful += 1
        
        summary = CompetitionSummary(
            competition_name=name,
            competition_type=comp_type,
            start_date=start_date,
            end_date=end_date,
            deadline=deadline,
            participants_count=competition_data.get("participants", 1000),
            final_ranking=competition_data.get("final_ranking"),
            final_percentile=competition_data.get("final_percentile"),
            medal_achieved=competition_data.get("medal"),
            initial_score=all_scores[0] if all_scores else 0.0,
            best_score=max(all_scores) if all_scores else 0.0,
            final_score=all_scores[-1] if all_scores else 0.0,
            score_improvement=max(all_scores) - all_scores[0] if len(all_scores) > 1 else 0.0,
            total_gpu_hours_used=total_gpu_hours,
            total_api_calls=sum(getattr(h, 'api_calls_count', 0) for h in analyzer_history),
            total_experiments=total_experiments,
            techniques_identified=techniques_identified,
            techniques_attempted=techniques_attempted,
            techniques_successful=techniques_successful
        )
        
        return summary
    
    async def _analyze_techniques(
        self,
        executor_history: List[Any],
        analysis_depth: RetrospectiveDepth
    ) -> List[TechniqueAnalysis]:
        """æŠ€è¡“åˆ†æ"""
        
        technique_stats = {}
        
        # å®Ÿè¡Œå±¥æ­´ã‹ã‚‰æŠ€è¡“åˆ¥çµ±è¨ˆåé›†
        for execution in executor_history:
            if not hasattr(execution, 'techniques_to_implement'):
                continue
                
            for technique_info in execution.techniques_to_implement:
                technique_name = technique_info.get("technique", "unknown")
                
                if technique_name not in technique_stats:
                    technique_stats[technique_name] = {
                        "attempts": 0,
                        "successes": 0,
                        "failures": 0,
                        "scores": [],
                        "execution_times": [],
                        "gpu_hours": [],
                        "failure_reasons": [],
                        "success_factors": []
                    }
                
                stats = technique_stats[technique_name]
                stats["attempts"] += 1
                
                # æˆåŠŸ/å¤±æ•—åˆ¤å®š
                execution_success = getattr(execution, 'success_rate', 0) > 0.5
                if execution_success:
                    stats["successes"] += 1
                    if hasattr(execution, 'best_score'):
                        stats["scores"].append(execution.best_score)
                    stats["success_factors"].append("Execution completed successfully")
                else:
                    stats["failures"] += 1
                    stats["failure_reasons"].append("Low success rate")
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                if hasattr(execution, 'execution_duration'):
                    stats["execution_times"].append(execution.execution_duration)
                if hasattr(execution, 'total_gpu_hours_used'):
                    stats["gpu_hours"].append(execution.total_gpu_hours_used)
        
        # æŠ€è¡“åˆ†æçµæœç”Ÿæˆ
        analyses = []
        for technique_name, stats in technique_stats.items():
            
            success_rate = stats["successes"] / max(stats["attempts"], 1)
            avg_score = statistics.mean(stats["scores"]) if stats["scores"] else 0.0
            avg_execution_time = statistics.mean(stats["execution_times"]) if stats["execution_times"] else 0.0
            total_gpu = sum(stats["gpu_hours"])
            
            # æœ‰åŠ¹æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
            effectiveness = self._calculate_technique_effectiveness(
                success_rate, avg_score, avg_execution_time, total_gpu
            )
            
            # é›£æ˜“åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            difficulty = 1.0 - success_rate  # æˆåŠŸç‡ãŒä½ã„ã»ã©é›£æ˜“åº¦é«˜
            
            # ROIã‚¹ã‚³ã‚¢è¨ˆç®—
            roi = avg_score / max(total_gpu, 0.1)  # ã‚¹ã‚³ã‚¢æ”¹å–„/GPUæ™‚é–“
            
            analysis = TechniqueAnalysis(
                technique_name=technique_name,
                category=self._categorize_technique(technique_name),
                attempts_count=stats["attempts"],
                success_count=stats["successes"],
                failure_count=stats["failures"],
                success_rate=success_rate,
                avg_score_improvement=avg_score,
                best_score_achieved=max(stats["scores"]) if stats["scores"] else 0.0,
                avg_execution_time_hours=avg_execution_time,
                total_gpu_hours_used=total_gpu,
                effectiveness_score=effectiveness,
                difficulty_score=difficulty,
                roi_score=roi,
                common_failures=stats["failure_reasons"][:3],
                success_factors=stats["success_factors"][:3]
            )
            
            analyses.append(analysis)
        
        # æœ‰åŠ¹æ€§é †ã§ã‚½ãƒ¼ãƒˆ
        analyses.sort(key=lambda a: a.effectiveness_score, reverse=True)
        
        return analyses
    
    def _calculate_technique_effectiveness(
        self,
        success_rate: float,
        avg_score: float,
        avg_time: float,
        total_gpu: float
    ) -> float:
        """æŠ€è¡“æœ‰åŠ¹æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        # å„è¦ç´ ã‚’0-1ã«æ­£è¦åŒ–
        success_component = success_rate
        score_component = min(avg_score / 0.9, 1.0)  # 0.9ã‚’æœ€å¤§ã¨ä»®å®š
        time_efficiency = max(0, 1.0 - avg_time / 24.0)  # 24æ™‚é–“ã‚’æœ€å¤§ã¨ä»®å®š
        gpu_efficiency = max(0, 1.0 - total_gpu / 10.0)  # 10æ™‚é–“ã‚’æœ€å¤§ã¨ä»®å®š
        
        # é‡ã¿ä»˜ãåˆè¨ˆ
        effectiveness = (
            success_component * 0.4 +
            score_component * 0.3 +
            time_efficiency * 0.15 +
            gpu_efficiency * 0.15
        )
        
        return min(effectiveness, 1.0)
    
    def _categorize_technique(self, technique_name: str) -> str:
        """æŠ€è¡“ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚º"""
        
        name_lower = technique_name.lower()
        
        if "ensemble" in name_lower or "boosting" in name_lower:
            return "ensemble"
        elif "stacking" in name_lower or "blending" in name_lower:
            return "stacking"
        elif "neural" in name_lower or "deep" in name_lower:
            return "deep_learning"
        elif "feature" in name_lower:
            return "feature_engineering"
        elif "optimization" in name_lower or "optuna" in name_lower:
            return "hyperparameter_optimization"
        else:
            return "other"
    
    async def _extract_learning_insights(
        self,
        competition_summary: CompetitionSummary,
        technique_analyses: List[TechniqueAnalysis],
        agent_histories: Dict[str, List[Any]],
        analysis_depth: RetrospectiveDepth
    ) -> List[LearningInsight]:
        """å­¦ç¿’çŸ¥è¦‹æŠ½å‡º"""
        
        insights = []
        
        # 1. æŠ€è¡“æœ‰åŠ¹æ€§ã«é–¢ã™ã‚‹çŸ¥è¦‹
        top_techniques = [t for t in technique_analyses if t.effectiveness_score > 0.7]
        if top_techniques:
            insight = LearningInsight(
                insight_id=f"insight-tech-eff-{uuid.uuid4().hex[:6]}",
                category=LearningCategory.TECHNIQUE_EFFECTIVENESS,
                title=f"é«˜åŠ¹æœæŠ€è¡“ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹: {competition_summary.competition_type}ç«¶æŠ€",
                description=f"ä»¥ä¸‹ã®æŠ€è¡“ãŒ{competition_summary.competition_type}ç«¶æŠ€ã§é«˜ã„åŠ¹æœã‚’ç™ºæ®: {', '.join([t.technique_name for t in top_techniques[:3]])}",
                supporting_data={
                    "techniques": [t.technique_name for t in top_techniques],
                    "avg_effectiveness": statistics.mean([t.effectiveness_score for t in top_techniques]),
                    "avg_success_rate": statistics.mean([t.success_rate for t in top_techniques])
                },
                confidence_level=min(statistics.mean([t.success_rate for t in top_techniques]), 1.0),
                statistical_significance=0.05,  # ä»®ã®å€¤
                applicable_competition_types=[competition_summary.competition_type],
                applicable_contexts=["similar_dataset_size", "similar_timeline"],
                improvement_recommendations=[
                    f"æ¬¡å›{competition_summary.competition_type}ç«¶æŠ€ã§ã¯{top_techniques[0].technique_name}ã‚’å„ªå…ˆå®Ÿè£…",
                    "é«˜åŠ¹æœæŠ€è¡“ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã«é‡ç‚¹é…åˆ†"
                ],
                priority=ImprovementPriority.HIGH,
                discovered_at=datetime.utcnow()
            )
            insights.append(insight)
        
        # 2. ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–ã«é–¢ã™ã‚‹çŸ¥è¦‹
        if competition_summary.total_gpu_hours_used > 0:
            gpu_efficiency = competition_summary.score_improvement / competition_summary.total_gpu_hours_used
            
            if gpu_efficiency > 0.01:  # 1GPUæ™‚é–“ã‚ãŸã‚Š0.01ä»¥ä¸Šã®ã‚¹ã‚³ã‚¢æ”¹å–„
                insight = LearningInsight(
                    insight_id=f"insight-resource-{uuid.uuid4().hex[:6]}",
                    category=LearningCategory.RESOURCE_OPTIMIZATION,
                    title="åŠ¹ç‡çš„GPUæ´»ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç¢ºèª",
                    description=f"GPUæ™‚é–“å½“ãŸã‚Šã‚¹ã‚³ã‚¢æ”¹å–„åŠ¹ç‡: {gpu_efficiency:.4f}",
                    supporting_data={
                        "gpu_hours_used": competition_summary.total_gpu_hours_used,
                        "score_improvement": competition_summary.score_improvement,
                        "efficiency": gpu_efficiency
                    },
                    confidence_level=0.8,
                    statistical_significance=0.05,
                    applicable_competition_types=[competition_summary.competition_type],
                    applicable_contexts=["limited_gpu_budget"],
                    improvement_recommendations=[
                        "åŒæ§˜ã®åŠ¹ç‡æ€§ã‚’æ¬¡å›ã‚‚ç¶­æŒ",
                        "æ›´ãªã‚‹åŠ¹ç‡åŒ–ã®ä½™åœ°ã‚’æ¢ç´¢"
                    ],
                    priority=ImprovementPriority.MEDIUM,
                    discovered_at=datetime.utcnow()
                )
                insights.append(insight)
        
        # 3. ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«é–¢ã™ã‚‹çŸ¥è¦‹
        failed_techniques = [t for t in technique_analyses if t.success_rate < 0.5]
        if failed_techniques:
            common_failures = []
            for tech in failed_techniques:
                common_failures.extend(tech.common_failures)
            
            if common_failures:
                insight = LearningInsight(
                    insight_id=f"insight-error-{uuid.uuid4().hex[:6]}",
                    category=LearningCategory.ERROR_PATTERNS,
                    title="å…±é€šå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ç‰¹å®š",
                    description=f"ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¤‡æ•°æŠ€è¡“ã§ç™ºç”Ÿ: {', '.join(set(common_failures[:3]))}",
                    supporting_data={
                        "failed_techniques": [t.technique_name for t in failed_techniques],
                        "common_failures": list(set(common_failures)),
                        "failure_frequency": len(common_failures)
                    },
                    confidence_level=0.7,
                    statistical_significance=0.05,
                    applicable_competition_types=[competition_summary.competition_type],
                    applicable_contexts=["similar_complexity"],
                    improvement_recommendations=[
                        "äº‹å‰ã«ã“ã‚Œã‚‰ã®ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã¸ã®å¯¾ç­–ã‚’æº–å‚™",
                        "å¤±æ•—ã—ã‚„ã™ã„æŠ€è¡“ã«ã¯è¿½åŠ ã®ãƒ†ã‚¹ãƒˆæ™‚é–“ã‚’ç¢ºä¿"
                    ],
                    priority=ImprovementPriority.HIGH,
                    discovered_at=datetime.utcnow()
                )
                insights.append(insight)
        
        return insights
    
    async def _validate_existing_insights(
        self,
        competition_summary: CompetitionSummary,
        technique_analyses: List[TechniqueAnalysis]
    ) -> List[LearningInsight]:
        """æ—¢å­˜çŸ¥è¦‹æ¤œè¨¼"""
        
        validated_insights = []
        
        for insight in self.historical_insights:
            if insight.validated:
                continue
                
            # é©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
            if competition_summary.competition_type not in insight.applicable_competition_types:
                continue
            
            # æ¤œè¨¼å®Ÿè¡Œ
            validation_result = await self._validate_insight(
                insight, competition_summary, technique_analyses
            )
            
            if validation_result["confirmed"]:
                insight.validated = True
                insight.applied_count += 1
                validated_insights.append(insight)
                
                self.logger.info(f"çŸ¥è¦‹æ¤œè¨¼æˆåŠŸ: {insight.title}")
        
        return validated_insights
    
    async def _validate_insight(
        self,
        insight: LearningInsight,
        competition_summary: CompetitionSummary,
        technique_analyses: List[TechniqueAnalysis]
    ) -> Dict[str, Any]:
        """å€‹åˆ¥çŸ¥è¦‹æ¤œè¨¼"""
        
        if insight.category == LearningCategory.TECHNIQUE_EFFECTIVENESS:
            # æŠ€è¡“æœ‰åŠ¹æ€§çŸ¥è¦‹ã®æ¤œè¨¼
            recommended_techniques = insight.supporting_data.get("techniques", [])
            
            for tech_analysis in technique_analyses:
                if (tech_analysis.technique_name in recommended_techniques and
                    tech_analysis.effectiveness_score >= insight.confidence_level):
                    return {
                        "confirmed": True,
                        "evidence": f"{tech_analysis.technique_name}ãŒæœŸå¾…é€šã‚Šã®åŠ¹æœã‚’ç™ºæ®",
                        "confidence": tech_analysis.effectiveness_score
                    }
        
        elif insight.category == LearningCategory.RESOURCE_OPTIMIZATION:
            # ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–çŸ¥è¦‹ã®æ¤œè¨¼
            expected_efficiency = insight.supporting_data.get("efficiency", 0)
            actual_efficiency = (
                competition_summary.score_improvement / 
                max(competition_summary.total_gpu_hours_used, 0.1)
            )
            
            if actual_efficiency >= expected_efficiency * 0.8:  # 80%ä»¥ä¸Šã®åŠ¹ç‡é”æˆ
                return {
                    "confirmed": True,
                    "evidence": f"æœŸå¾…åŠ¹ç‡{expected_efficiency:.4f}ã«å¯¾ã—{actual_efficiency:.4f}ã‚’é”æˆ",
                    "confidence": min(actual_efficiency / expected_efficiency, 1.0)
                }
        
        return {
            "confirmed": False,
            "evidence": "æ¤œè¨¼æ¡ä»¶ã‚’æº€ãŸã•ãš",
            "confidence": 0.0
        }
    
    async def _analyze_resource_efficiency(
        self,
        agent_histories: Dict[str, List[Any]],
        competition_summary: CompetitionSummary
    ) -> Dict[str, Any]:
        """ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡æ€§åˆ†æ"""
        
        # GPUåŠ¹ç‡æ€§
        gpu_efficiency = 0.0
        if competition_summary.total_gpu_hours_used > 0:
            gpu_efficiency = competition_summary.score_improvement / competition_summary.total_gpu_hours_used
        
        # æ™‚é–“åŠ¹ç‡æ€§
        total_competition_hours = (competition_summary.end_date - competition_summary.start_date).total_seconds() / 3600
        time_utilization = competition_summary.total_gpu_hours_used / max(total_competition_hours, 1)
        
        # APIåŠ¹ç‡æ€§
        api_efficiency = 0.0
        if competition_summary.total_api_calls > 0:
            api_efficiency = competition_summary.techniques_identified / competition_summary.total_api_calls
        
        # å®Ÿé¨“åŠ¹ç‡æ€§
        experiment_efficiency = 0.0
        if competition_summary.total_experiments > 0:
            experiment_efficiency = competition_summary.techniques_successful / competition_summary.total_experiments
        
        efficiency_analysis = {
            "gpu_efficiency": gpu_efficiency,
            "time_utilization": time_utilization,
            "api_efficiency": api_efficiency,
            "experiment_efficiency": experiment_efficiency
        }
        
        # æ™‚é–“é…åˆ†åˆ†æ
        time_allocation = {
            "analysis_phase": 0.2,      # åˆ†æ20%
            "implementation_phase": 0.6, # å®Ÿè£…60%
            "optimization_phase": 0.2    # æœ€é©åŒ–20%
        }
        
        return {
            "efficiency": efficiency_analysis,
            "time_allocation": time_allocation
        }
    
    async def _generate_improvement_recommendations(
        self,
        competition_summary: CompetitionSummary,
        technique_analyses: List[TechniqueAnalysis],
        new_insights: List[LearningInsight]
    ) -> Dict[str, List[str]]:
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        
        achievements = []
        failures = []
        immediate_improvements = []
        strategic_improvements = []
        lessons = []
        best_practices = []
        
        # æˆæœã®ç‰¹å®š
        if competition_summary.score_improvement > 0.05:
            achievements.append(f"å¤§å¹…ã‚¹ã‚³ã‚¢æ”¹å–„é”æˆ: {competition_summary.score_improvement:.4f}")
        if competition_summary.medal_achieved:
            achievements.append(f"ãƒ¡ãƒ€ãƒ«ç²å¾—: {competition_summary.medal_achieved}")
        
        high_performing_techniques = [t for t in technique_analyses if t.effectiveness_score > 0.8]
        if high_performing_techniques:
            achievements.append(f"é«˜åŠ¹æœæŠ€è¡“ã®æˆåŠŸå®Ÿè£…: {', '.join([t.technique_name for t in high_performing_techniques[:2]])}")
        
        # å¤±æ•—ã®ç‰¹å®š
        if competition_summary.score_improvement < 0.01:
            failures.append("ã‚¹ã‚³ã‚¢æ”¹å–„ä¸è¶³")
        if competition_summary.techniques_successful / max(competition_summary.techniques_attempted, 1) < 0.5:
            failures.append("æŠ€è¡“å®Ÿè£…æˆåŠŸç‡ä½ä¸‹")
        
        low_performing_techniques = [t for t in technique_analyses if t.effectiveness_score < 0.3]
        if low_performing_techniques:
            failures.append(f"ä½åŠ¹æœæŠ€è¡“ã®é¸æŠ: {', '.join([t.technique_name for t in low_performing_techniques[:2]])}")
        
        # å³åº§æ”¹å–„äº‹é …
        for technique in low_performing_techniques:
            immediate_improvements.append(f"{technique.technique_name}ã®å®Ÿè£…æ–¹æ³•è¦‹ç›´ã—")
        
        if competition_summary.total_gpu_hours_used > 40:  # 40æ™‚é–“ä»¥ä¸Šä½¿ç”¨
            immediate_improvements.append("GPUä½¿ç”¨é‡ã®æœ€é©åŒ–")
        
        # æˆ¦ç•¥çš„æ”¹å–„äº‹é …
        for insight in new_insights:
            if insight.priority in [ImprovementPriority.CRITICAL, ImprovementPriority.HIGH]:
                strategic_improvements.extend(insight.improvement_recommendations)
        
        # æ¬¡å›ã¸ã®æ•™è¨“
        lessons.extend([
            f"æœ€å„ªå…ˆæŠ€è¡“: {technique_analyses[0].technique_name if technique_analyses else 'ãªã—'}",
            f"GPUäºˆç®—é…åˆ†: {competition_summary.total_gpu_hours_used:.1f}æ™‚é–“ãŒé©æ­£"
        ])
        
        # ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æ›´æ–°
        for technique in high_performing_techniques:
            best_practices.append(f"{technique.technique_name}: {', '.join(technique.success_factors[:2])}")
        
        return {
            "achievements": achievements,
            "failures": failures,
            "immediate": immediate_improvements,
            "strategic": strategic_improvements,
            "lessons": lessons,
            "best_practices": best_practices
        }
    
    def _evaluate_overall_performance(self, competition_summary: CompetitionSummary) -> str:
        """å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡"""
        
        score = 0
        
        # ã‚¹ã‚³ã‚¢æ”¹å–„
        if competition_summary.score_improvement > 0.1:
            score += 3
        elif competition_summary.score_improvement > 0.05:
            score += 2
        elif competition_summary.score_improvement > 0.01:
            score += 1
        
        # ãƒ¡ãƒ€ãƒ«ç²å¾—
        if competition_summary.medal_achieved == "gold":
            score += 3
        elif competition_summary.medal_achieved == "silver":
            score += 2
        elif competition_summary.medal_achieved == "bronze":
            score += 1
        
        # æŠ€è¡“æˆåŠŸç‡
        if competition_summary.techniques_attempted > 0:
            success_rate = competition_summary.techniques_successful / competition_summary.techniques_attempted
            if success_rate > 0.8:
                score += 2
            elif success_rate > 0.6:
                score += 1
        
        # è©•ä¾¡å¤‰æ›
        if score >= 6:
            return "excellent"
        elif score >= 4:
            return "good"
        elif score >= 2:
            return "fair"
        else:
            return "poor"
    
    async def _update_knowledge_base(self, report: RetrospectiveReport):
        """çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°"""
        
        # æ–°ã—ã„çŸ¥è¦‹ã‚’è¿½åŠ 
        self.historical_insights.extend(report.new_insights)
        
        # æŠ€è¡“çŸ¥è­˜æ›´æ–°
        for analysis in report.technique_analyses:
            if analysis.technique_name in self.technique_knowledge:
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ¼ã‚¸
                existing = self.technique_knowledge[analysis.technique_name]
                # ç°¡å˜ãªå¹³å‡åŒ–ï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸæ›´æ–°ãŒå¿…è¦ï¼‰
                existing.effectiveness_score = (existing.effectiveness_score + analysis.effectiveness_score) / 2
                existing.attempts_count += analysis.attempts_count
            else:
                self.technique_knowledge[analysis.technique_name] = analysis
        
        # ç«¶æŠ€ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°
        comp_type = report.competition_summary.competition_type
        if comp_type not in self.competition_patterns:
            self.competition_patterns[comp_type] = []
        
        self.competition_patterns[comp_type].append({
            "score_improvement": report.competition_summary.score_improvement,
            "gpu_hours_used": report.competition_summary.total_gpu_hours_used,
            "success_rate": report.competition_summary.techniques_successful / max(report.competition_summary.techniques_attempted, 1),
            "date": report.generated_at
        })
        
        self.logger.info(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°å®Œäº†: {len(report.new_insights)}æ–°çŸ¥è¦‹è¿½åŠ ")
    
    async def _create_retrospective_issue(
        self,
        competition_data: Dict[str, Any],
        analysis_depth: RetrospectiveDepth
    ) -> int:
        """æŒ¯ã‚Šè¿”ã‚ŠIssueä½œæˆ"""
        
        title = f"ğŸ” ç«¶æŠ€æŒ¯ã‚Šè¿”ã‚Šåˆ†æ: {competition_data.get('name', 'Unknown')}"
        
        description = f"""
## ç«¶æŠ€æŒ¯ã‚Šè¿”ã‚Šåˆ†æé–‹å§‹

**ç«¶æŠ€å**: {competition_data.get('name', 'Unknown')}
**åˆ†æã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆID**: `{self.agent_id}`
**åˆ†ææ·±åº¦**: {analysis_depth.value}
**é–‹å§‹æ™‚åˆ»**: {datetime.utcnow().isoformat()}

### åˆ†æé …ç›®
- ç«¶æŠ€å…¨ä½“ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
- æŠ€è¡“åˆ¥åŠ¹æœåˆ†æ
- å­¦ç¿’çŸ¥è¦‹æŠ½å‡ºãƒ»æ¤œè¨¼
- ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡æ€§è©•ä¾¡
- æ”¹å–„ææ¡ˆç”Ÿæˆ
- çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ›´æ–°

ã“ã®Issueã§æŒ¯ã‚Šè¿”ã‚Šåˆ†æã®é€²æ—ã¨çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚
        """
        
        try:
            issue_data = await self.atomic_operations.create_issue(
                title=title,
                description=description,
                labels=["retrospective", "analysis", "learning"]
            )
            return issue_data["number"]
            
        except Exception as e:
            self.logger.error(f"æŒ¯ã‚Šè¿”ã‚ŠIssueä½œæˆå¤±æ•—: {e}")
            return -1
    
    async def _post_retrospective_report(self, report: RetrospectiveReport, issue_number: int):
        """æŒ¯ã‚Šè¿”ã‚Šãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿"""
        
        performance_emoji = {
            "excellent": "ğŸ†",
            "good": "âœ…", 
            "fair": "âš ï¸",
            "poor": "âŒ"
        }
        
        emoji = performance_emoji.get(report.overall_performance, "ğŸ“Š")
        
        content = f"""
## {emoji} ç«¶æŠ€æŒ¯ã‚Šè¿”ã‚Šåˆ†æå®Œäº†

### ğŸ“ˆ å…¨ä½“è©•ä¾¡: {report.overall_performance.upper()}

### ğŸ¯ ç«¶æŠ€ã‚µãƒãƒªãƒ¼
- **ç«¶æŠ€å**: {report.competition_summary.competition_name}
- **ç«¶æŠ€ã‚¿ã‚¤ãƒ—**: {report.competition_summary.competition_type}
- **ã‚¹ã‚³ã‚¢æ”¹å–„**: {report.competition_summary.score_improvement:.4f}
- **æœ€é«˜ã‚¹ã‚³ã‚¢**: {report.competition_summary.best_score:.4f}
- **GPUä½¿ç”¨æ™‚é–“**: {report.competition_summary.total_gpu_hours_used:.1f}æ™‚é–“
- **å®Ÿé¨“æ•°**: {report.competition_summary.total_experiments}
- **æŠ€è¡“æˆåŠŸç‡**: {report.competition_summary.techniques_successful}/{report.competition_summary.techniques_attempted}

### ğŸ† ä¸»ãªæˆæœ
{chr(10).join([f"- {achievement}" for achievement in report.key_achievements]) if report.key_achievements else "- ãªã—"}

### âŒ ä¸»ãªèª²é¡Œ
{chr(10).join([f"- {failure}" for failure in report.major_failures]) if report.major_failures else "- ãªã—"}

### ğŸ”¬ æŠ€è¡“åˆ†æçµæœ
{chr(10).join([f"- **{t.technique_name}**: æœ‰åŠ¹æ€§{t.effectiveness_score:.2f}, æˆåŠŸç‡{t.success_rate:.1%}" for t in report.technique_analyses[:5]]) if report.technique_analyses else "- åˆ†æãƒ‡ãƒ¼ã‚¿ãªã—"}

### ğŸ’¡ æ–°ç™ºè¦‹çŸ¥è¦‹
{chr(10).join([f"- **{i.title}**: {i.description}" for i in report.new_insights]) if report.new_insights else "- æ–°çŸ¥è¦‹ãªã—"}

### ğŸ¯ å³åº§æ”¹å–„äº‹é …
{chr(10).join([f"- {improvement}" for improvement in report.immediate_improvements]) if report.immediate_improvements else "- ãªã—"}

### ğŸ“š æ¬¡å›ã¸ã®æ•™è¨“
{chr(10).join([f"- {lesson}" for lesson in report.lessons_for_next_competition]) if report.lessons_for_next_competition else "- ãªã—"}

### ğŸ“Š ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡æ€§
- **GPUåŠ¹ç‡**: {report.resource_efficiency.get('gpu_efficiency', 0):.4f}
- **æ™‚é–“æ´»ç”¨ç‡**: {report.resource_efficiency.get('time_utilization', 0):.2f}
- **å®Ÿé¨“åŠ¹ç‡**: {report.resource_efficiency.get('experiment_efficiency', 0):.2f}

---
*ãƒ¬ãƒãƒ¼ãƒˆID: {report.report_id} | Retrospective Agent {self.agent_id}*
        """
        
        try:
            await self.atomic_operations.create_comment(
                issue_number=issue_number,
                comment_body=content
            )
            self.logger.info(f"æŒ¯ã‚Šè¿”ã‚Šãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿å®Œäº†: Issue #{issue_number}")
            
        except Exception as e:
            self.logger.error(f"æŒ¯ã‚Šè¿”ã‚Šãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿å¤±æ•—: {e}")
    
    async def _post_error_report(self, error: Exception, issue_number: int):
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿"""
        
        content = f"""
## âŒ æŒ¯ã‚Šè¿”ã‚Šåˆ†æã‚¨ãƒ©ãƒ¼

**ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚åˆ»**: {datetime.utcnow().isoformat()}
**ã‚¨ãƒ©ãƒ¼å†…å®¹**: {str(error)}

æŒ¯ã‚Šè¿”ã‚Šåˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã«ç¢ºèªã‚’ä¾é ¼ã—ã¦ãã ã•ã„ã€‚

---
*Retrospective Agent {self.agent_id}*
        """
        
        try:
            await self.atomic_operations.create_comment(
                issue_number=issue_number,
                comment_body=content
            )
        except Exception as e:
            self.logger.error(f"ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿å¤±æ•—: {e}")
    
    async def get_learning_summary(self) -> Dict[str, Any]:
        """å­¦ç¿’ã‚µãƒãƒªãƒ¼å–å¾—"""
        
        return {
            "agent_id": self.agent_id,
            "total_retrospectives": len(self.retrospective_history),
            "total_insights": len(self.historical_insights),
            "validated_insights": len([i for i in self.historical_insights if i.validated]),
            "technique_knowledge_count": len(self.technique_knowledge),
            "competition_patterns_tracked": len(self.competition_patterns),
            "most_effective_techniques": sorted(
                self.technique_knowledge.items(),
                key=lambda x: x[1].effectiveness_score,
                reverse=True
            )[:3],
            "recent_learning_insights": [
                {
                    "title": insight.title,
                    "category": insight.category.value,
                    "confidence": insight.confidence_level,
                    "priority": insight.priority.value
                }
                for insight in self.historical_insights[-5:]
            ]
        }