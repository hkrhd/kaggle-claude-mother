#!/usr/bin/env python3
"""
LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­ã‚·ã‚¹ãƒ†ãƒ å˜ä½“ãƒ†ã‚¹ãƒˆ

ExecutorAgentã®æå‡ºåˆ¤æ–­LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å˜ä½“ãƒ†ã‚¹ãƒˆãƒ»å‹•ä½œç¢ºèª
"""

import asyncio
import sys
import os
import logging
from datetime import datetime, timedelta

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'system'))

# LLMãƒ™ãƒ¼ã‚¹åˆ¤æ–­ã‚·ã‚¹ãƒ†ãƒ 
from system.agents.shared.llm_decision_base import ClaudeClient
from system.agents.executor.submission_decision_agent import SubmissionDecisionAgent, SubmissionContext


async def test_llm_basic_submission_decision():
    """åŸºæœ¬çš„ãªLLMæå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ¤– åŸºæœ¬LLMæå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # Claude ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        claude_client = ClaudeClient()
        
        # æå‡ºåˆ¤æ–­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
        submission_agent = SubmissionDecisionAgent(claude_client)
        
        # ãƒ†ã‚¹ãƒˆç”¨æå‡ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆé«˜ã‚¹ã‚³ã‚¢ãƒ»ãƒ¡ãƒ€ãƒ«åœå†…ï¼‰
        high_performance_context = SubmissionContext(
            competition_name="High Performance Test Competition",
            current_best_score=0.9234,
            target_score=0.9100,
            current_rank_estimate=25,
            total_participants=3500,
            days_remaining=2,
            hours_remaining=48.0,
            
            experiments_completed=45,
            experiments_running=1,
            success_rate=0.85,
            resource_budget_remaining=0.15,
            
            score_history=[0.8987, 0.9045, 0.9123, 0.9189, 0.9234],
            score_improvement_trend=0.0045,
            plateau_duration_hours=3.2,
            
            leaderboard_top10_scores=[
                0.9567, 0.9432, 0.9389, 0.9278, 0.9245,
                0.9234, 0.9201, 0.9178, 0.9156, 0.9134
            ],
            medal_threshold_estimate=0.9200,
            current_medal_zone="silver",
            
            model_stability=0.92,
            overfitting_risk=0.15,
            technical_debt_level=0.20
        )
        
        # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çŠ¶æ³ã§ã®åˆ¤æ–­ãƒ†ã‚¹ãƒˆ
        logger.info("ğŸ“Š é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çŠ¶æ³ãƒ†ã‚¹ãƒˆ")
        
        decision_response = await submission_agent.should_submit_competition(
            context=high_performance_context,
            urgency="medium"
        )
        
        decision = decision_response.decision_result
        
        logger.info(f"  åˆ¤æ–­çµæœ: {decision['decision']}")
        logger.info(f"  ä¿¡é ¼åº¦: {decision_response.confidence_score:.2f}")
        logger.info(f"  å®Ÿè¡Œæ™‚é–“: {decision_response.execution_time_seconds:.1f}ç§’")
        logger.info(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨: {decision_response.fallback_used}")
        logger.info(f"  æ¨è«–: {decision.get('reasoning', 'N/A')[:100]}...")
        
        medal_prob = decision.get('medal_probability', {})
        logger.info(f"  ãƒ¡ãƒ€ãƒ«ç¢ºç‡: Gold={medal_prob.get('gold', 0):.2f}, Silver={medal_prob.get('silver', 0):.2f}")
        
        # äºˆæƒ³: é«˜ã‚¹ã‚³ã‚¢+ãƒ¡ãƒ€ãƒ«åœãªã®ã§ SUBMIT ã®å¯èƒ½æ€§ãŒé«˜ã„
        expected_decision = "SUBMIT"
        actual_decision = decision['decision']
        
        if actual_decision == expected_decision:
            logger.info("  âœ… æœŸå¾…ã•ã‚Œã‚‹åˆ¤æ–­çµæœ")
        else:
            logger.warning(f"  âš ï¸ äºˆæƒ³å¤–ã®åˆ¤æ–­: æœŸå¾…={expected_decision}, å®Ÿéš›={actual_decision}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ åŸºæœ¬LLMæå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_llm_different_scenarios():
    """ç•°ãªã‚‹ã‚·ãƒŠãƒªã‚ªã§ã®LLMæå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ¯ ã‚·ãƒŠãƒªã‚ªåˆ¥LLMæå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        claude_client = ClaudeClient()
        submission_agent = SubmissionDecisionAgent(claude_client)
        
        # ã‚·ãƒŠãƒªã‚ª1: ä½ã‚¹ã‚³ã‚¢ãƒ»æ™‚é–“åˆ‡è¿«
        logger.info("ğŸ“‰ ã‚·ãƒŠãƒªã‚ª1: ä½ã‚¹ã‚³ã‚¢ãƒ»æ™‚é–“åˆ‡è¿«")
        
        low_score_context = SubmissionContext(
            competition_name="Low Score Emergency",
            current_best_score=0.7234,
            target_score=0.8500,
            current_rank_estimate=800,
            total_participants=2000,
            days_remaining=0,
            hours_remaining=6.0,  # 6æ™‚é–“ã—ã‹ãªã„
            
            experiments_completed=15,
            experiments_running=2,
            success_rate=0.60,
            resource_budget_remaining=0.80,
            
            score_history=[0.6890, 0.7012, 0.7123, 0.7198, 0.7234],
            score_improvement_trend=0.0036,
            plateau_duration_hours=8.0,
            
            leaderboard_top10_scores=[
                0.8567, 0.8432, 0.8389, 0.8278, 0.8245,
                0.8234, 0.8201, 0.8178, 0.8156, 0.8134
            ],
            medal_threshold_estimate=0.8200,
            current_medal_zone="none",
            
            model_stability=0.65,
            overfitting_risk=0.45,
            technical_debt_level=0.50
        )
        
        decision1 = await submission_agent.should_submit_competition(
            context=low_score_context,
            urgency="critical"
        )
        
        logger.info(f"  åˆ¤æ–­: {decision1.decision_result['decision']} (ä¿¡é ¼åº¦: {decision1.confidence_score:.2f})")
        logger.info(f"  æ¨è«–: {decision1.decision_result.get('reasoning', 'N/A')[:80]}...")
        
        # ã‚·ãƒŠãƒªã‚ª2: ä¸­ç¨‹åº¦ã‚¹ã‚³ã‚¢ãƒ»æ”¹å–„ä¸­
        logger.info("ğŸ“ˆ ã‚·ãƒŠãƒªã‚ª2: ä¸­ç¨‹åº¦ã‚¹ã‚³ã‚¢ãƒ»æ”¹å–„ä¸­")
        
        improving_context = SubmissionContext(
            competition_name="Improving Performance",
            current_best_score=0.8456,
            target_score=0.8800,
            current_rank_estimate=200,
            total_participants=3000,
            days_remaining=7,
            hours_remaining=168.0,
            
            experiments_completed=30,
            experiments_running=3,
            success_rate=0.75,
            resource_budget_remaining=0.50,
            
            score_history=[0.8123, 0.8234, 0.8345, 0.8398, 0.8456],
            score_improvement_trend=0.0083,  # è‰¯ã„æ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰
            plateau_duration_hours=1.5,
            
            leaderboard_top10_scores=[
                0.8967, 0.8834, 0.8789, 0.8678, 0.8645,
                0.8634, 0.8601, 0.8578, 0.8556, 0.8534
            ],
            medal_threshold_estimate=0.8600,
            current_medal_zone="bronze",
            
            model_stability=0.85,
            overfitting_risk=0.25,
            technical_debt_level=0.30
        )
        
        decision2 = await submission_agent.should_submit_competition(
            context=improving_context,
            urgency="low"
        )
        
        logger.info(f"  åˆ¤æ–­: {decision2.decision_result['decision']} (ä¿¡é ¼åº¦: {decision2.confidence_score:.2f})")
        logger.info(f"  æ¨è«–: {decision2.decision_result.get('reasoning', 'N/A')[:80]}...")
        
        # ã‚·ãƒŠãƒªã‚ª3: é«˜ãƒªã‚¹ã‚¯çŠ¶æ³
        logger.info("âš ï¸ ã‚·ãƒŠãƒªã‚ª3: é«˜ãƒªã‚¹ã‚¯çŠ¶æ³")
        
        high_risk_context = SubmissionContext(
            competition_name="High Risk Situation",
            current_best_score=0.8789,
            target_score=0.8900,
            current_rank_estimate=120,
            total_participants=4000,
            days_remaining=3,
            hours_remaining=72.0,
            
            experiments_completed=60,
            experiments_running=0,
            success_rate=0.45,  # ä½ã„æˆåŠŸç‡
            resource_budget_remaining=0.05,  # ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡
            
            score_history=[0.8789, 0.8789, 0.8789, 0.8789, 0.8789],  # ã‚¹ã‚³ã‚¢åœæ»
            score_improvement_trend=0.0000,
            plateau_duration_hours=48.0,  # é•·æ™‚é–“åœæ»
            
            leaderboard_top10_scores=[
                0.9234, 0.9156, 0.9089, 0.9012, 0.8967,
                0.8934, 0.8901, 0.8878, 0.8856, 0.8834
            ],
            medal_threshold_estimate=0.8850,
            current_medal_zone="bronze",
            
            model_stability=0.50,  # ä¸å®‰å®š
            overfitting_risk=0.80,  # é«˜ã„éå­¦ç¿’ãƒªã‚¹ã‚¯
            technical_debt_level=0.70
        )
        
        decision3 = await submission_agent.should_submit_competition(
            context=high_risk_context,
            urgency="high"
        )
        
        logger.info(f"  åˆ¤æ–­: {decision3.decision_result['decision']} (ä¿¡é ¼åº¦: {decision3.confidence_score:.2f})")
        logger.info(f"  æ¨è«–: {decision3.decision_result.get('reasoning', 'N/A')[:80]}...")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆç¢ºèª
        perf_metrics = submission_agent.get_performance_metrics()
        logger.info("ğŸ“ˆ æå‡ºåˆ¤æ–­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±è¨ˆ:")
        logger.info(f"  ç·åˆ¤æ–­å›æ•°: {perf_metrics['total_decisions']}")
        logger.info(f"  æˆåŠŸç‡: {perf_metrics['success_rate']:.1%}")
        logger.info(f"  å¹³å‡å¿œç­”æ™‚é–“: {perf_metrics['average_response_time']:.1f}ç§’")
        logger.info(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‡: {perf_metrics['fallback_rate']:.1%}")
        
        # åˆ¤æ–­ã®å¤šæ§˜æ€§ç¢ºèª
        decisions = [
            decision1.decision_result['decision'],
            decision2.decision_result['decision'],
            decision3.decision_result['decision']
        ]
        
        unique_decisions = set(decisions)
        logger.info(f"ğŸ¯ åˆ¤æ–­å¤šæ§˜æ€§: {len(unique_decisions)}ç¨®é¡ã®åˆ¤æ–­ {list(unique_decisions)}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ã‚·ãƒŠãƒªã‚ªåˆ¥LLMæå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_fallback_mechanism():
    """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹ãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ›¡ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # æ•…æ„ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ãè¨­å®šã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’èª˜ç™º
        claude_client = ClaudeClient()
        submission_agent = SubmissionDecisionAgent(claude_client)
        
        # éå¸¸ã«çŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã®ãƒ†ã‚¹ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        test_context = SubmissionContext(
            competition_name="Fallback Test",
            current_best_score=0.8000,
            target_score=0.8500,
            current_rank_estimate=300,
            total_participants=2000,
            days_remaining=5,
            hours_remaining=120.0,
            
            experiments_completed=20,
            experiments_running=1,
            success_rate=0.70,
            resource_budget_remaining=0.40,
            
            score_history=[0.7800, 0.7900, 0.7950, 0.7980, 0.8000],
            score_improvement_trend=0.0020,
            plateau_duration_hours=5.0,
            
            leaderboard_top10_scores=[
                0.8800, 0.8700, 0.8600, 0.8500, 0.8450,
                0.8400, 0.8350, 0.8300, 0.8250, 0.8200
            ],
            medal_threshold_estimate=0.8300,
            current_medal_zone="none",
            
            model_stability=0.75,
            overfitting_risk=0.30,
            technical_debt_level=0.35
        )
        
        # æ¥µçŸ­ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
        from system.agents.shared.llm_decision_base import LLMDecisionRequest, LLMDecisionType
        
        fallback_request = LLMDecisionRequest(
            request_id="fallback-test",
            decision_type=LLMDecisionType.SUBMISSION_DECISION,
            context_data={},  # ç°¡æ˜“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            urgency_level="medium",
            fallback_strategy="conservative_submit",
            max_response_time_seconds=0.001  # æ¥µçŸ­ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
        fallback_decision = await submission_agent.make_decision(fallback_request)
        
        logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ¤æ–­çµæœ:")
        logger.info(f"  åˆ¤æ–­: {fallback_decision.decision_result.get('decision', 'N/A')}")
        logger.info(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨: {fallback_decision.fallback_used}")
        logger.info(f"  ä¿¡é ¼åº¦: {fallback_decision.confidence_score:.2f}")
        logger.info(f"  å®Ÿè¡Œæ™‚é–“: {fallback_decision.execution_time_seconds:.3f}ç§’")
        
        if fallback_decision.fallback_used:
            logger.info("  âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹æ­£å¸¸å‹•ä½œ")
            return True
        else:
            logger.warning("  âš ï¸ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒæœŸå¾…é€šã‚Šå‹•ä½œã—ãªã‹ã£ãŸ")
            return False
        
    except Exception as e:
        logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸ§ª LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­ã‚·ã‚¹ãƒ†ãƒ å˜ä½“ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    test_results = []
    
    # ãƒ†ã‚¹ãƒˆ1: åŸºæœ¬çš„ãªLLMæå‡ºåˆ¤æ–­
    print("\nğŸ¤– ãƒ†ã‚¹ãƒˆ1: åŸºæœ¬LLMæå‡ºåˆ¤æ–­")
    print("-" * 30)
    result1 = await test_llm_basic_submission_decision()
    test_results.append(("åŸºæœ¬LLMæå‡ºåˆ¤æ–­", result1))
    
    # ãƒ†ã‚¹ãƒˆ2: ç•°ãªã‚‹ã‚·ãƒŠãƒªã‚ªã§ã®åˆ¤æ–­
    print("\nğŸ¯ ãƒ†ã‚¹ãƒˆ2: ã‚·ãƒŠãƒªã‚ªåˆ¥åˆ¤æ–­")
    print("-" * 30)
    result2 = await test_llm_different_scenarios()
    test_results.append(("ã‚·ãƒŠãƒªã‚ªåˆ¥åˆ¤æ–­", result2))
    
    # ãƒ†ã‚¹ãƒˆ3: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹
    print("\nğŸ›¡ï¸ ãƒ†ã‚¹ãƒˆ3: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹")
    print("-" * 30)
    result3 = await test_fallback_mechanism()
    test_results.append(("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿæ§‹", result3))
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 50)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("-" * 30)
    
    passed = 0
    for test_name, result in test_results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ ãƒ†ã‚¹ãƒˆçµæœ: {passed}/{len(test_results)} PASS")
    
    if passed == len(test_results):
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ - LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œç¢ºèª")
        return 0
    else:
        print("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•— - ã‚·ã‚¹ãƒ†ãƒ ç¢ºèªãŒå¿…è¦")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)