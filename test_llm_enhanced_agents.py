#!/usr/bin/env python3
"""
LLMå¼·åŒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ

å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã®LLMãƒ™ãƒ¼ã‚¹åˆ¤æ–­ã‚·ã‚¹ãƒ†ãƒ ã®å‹•ä½œç¢ºèªãƒ»çµ±åˆãƒ†ã‚¹ãƒˆ
"""

import asyncio
import sys
import os
import logging
import subprocess
from datetime import datetime, timedelta

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'system'))

# LLMãƒ™ãƒ¼ã‚¹åˆ¤æ–­ã‚·ã‚¹ãƒ†ãƒ 
from system.agents.shared.llm_decision_base import ClaudeClient, CLAUDE_MODEL
from system.agents.executor.submission_decision_agent import SubmissionDecisionAgent, SubmissionContext
from system.agents.executor.executor_agent import ExecutorAgent, ExecutionRequest, ExecutionPriority

# ç«¶æŠ€é¸æŠã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆæ—¢å­˜ï¼‰
from system.agents.competition_selector.competition_selector_agent import (
    CompetitionSelectorAgent, SelectionStrategy
)

# ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
from system.agents.analyzer.analyzer_agent import AnalyzerAgent
from system.agents.monitor.monitor_agent import MonitorAgent


def get_github_token():
    """GitHubèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‹•çš„ã«å–å¾—"""
    try:
        # gh auth token ã‚³ãƒãƒ³ãƒ‰ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
        result = subprocess.run(
            ["gh", "auth", "token"],
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout.strip()
    except subprocess.CalledProcessError:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
        return os.environ.get("GITHUB_TOKEN", "test_token")


async def test_llm_submission_decision():
    """LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ¤– LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # Claude ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ï¼ˆãƒ¢ãƒ‡ãƒ«æŒ‡å®šä»˜ãï¼‰
        claude_client = ClaudeClient(model=CLAUDE_MODEL)
        
        # æå‡ºåˆ¤æ–­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
        submission_agent = SubmissionDecisionAgent(claude_client)
        
        # ãƒ†ã‚¹ãƒˆç”¨æå‡ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        test_context = SubmissionContext(
            competition_name="Test ML Competition",
            current_best_score=0.8756,
            target_score=0.9000,
            current_rank_estimate=87,
            total_participants=3500,
            days_remaining=5,
            hours_remaining=120.0,
            
            experiments_completed=25,
            experiments_running=3,
            success_rate=0.72,
            resource_budget_remaining=0.35,
            
            score_history=[0.8234, 0.8456, 0.8621, 0.8698, 0.8756],
            score_improvement_trend=0.0058,
            plateau_duration_hours=6.5,
            
            leaderboard_top10_scores=[
                0.9234, 0.9156, 0.9089, 0.9012, 0.8967,
                0.8934, 0.8901, 0.8878, 0.8856, 0.8834
            ],
            medal_threshold_estimate=0.8900,
            current_medal_zone="none",
            
            model_stability=0.85,
            overfitting_risk=0.25,
            technical_debt_level=0.30
        )
        
        # ç•°ãªã‚‹ç·Šæ€¥åº¦ã§ã®ãƒ†ã‚¹ãƒˆ
        urgency_levels = ["low", "medium", "high", "critical"]
        
        for urgency in urgency_levels:
            logger.info(f"ğŸ“Š æå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆ: ç·Šæ€¥åº¦{urgency}")
            
            decision_response = await submission_agent.should_submit_competition(
                context=test_context,
                urgency=urgency
            )
            
            decision = decision_response.decision_result
            
            logger.info(f"  åˆ¤æ–­çµæœ: {decision['decision']}")
            logger.info(f"  ä¿¡é ¼åº¦: {decision_response.confidence_score:.2f}")
            logger.info(f"  å®Ÿè¡Œæ™‚é–“: {decision_response.execution_time_seconds:.1f}ç§’")
            logger.info(f"  æ¨è«–: {decision.get('reasoning', 'N/A')[:100]}...")
            logger.info(f"  ãƒ¡ãƒ€ãƒ«ç¢ºç‡: Gold={decision.get('medal_probability', {}).get('gold', 0):.2f}")
            
            # æ±ºå®šã«å¿œã˜ãŸæ¨¡æ“¬å‡¦ç†
            if decision['decision'] == 'SUBMIT':
                logger.info("  â†’ âœ… æ¨¡æ“¬æå‡ºå®Ÿè¡Œ")
            elif decision['decision'] == 'CONTINUE':
                logger.info("  â†’ ğŸ”„ å®Ÿé¨“ç¶™ç¶š")
            else:
                logger.info("  â†’ â³ å¾…æ©Ÿ")
            
            logger.info("")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆç¢ºèª
        perf_metrics = submission_agent.get_performance_metrics()
        logger.info("ğŸ“ˆ æå‡ºåˆ¤æ–­ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±è¨ˆ:")
        logger.info(f"  ç·åˆ¤æ–­å›æ•°: {perf_metrics['total_decisions']}")
        logger.info(f"  æˆåŠŸç‡: {perf_metrics['success_rate']:.1%}")
        logger.info(f"  å¹³å‡å¿œç­”æ™‚é–“: {perf_metrics['average_response_time']:.1f}ç§’")
        logger.info(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‡: {perf_metrics['fallback_rate']:.1%}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ LLMæå‡ºåˆ¤æ–­ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_integrated_executor_agent():
    """LLMçµ±åˆExecutorAgentãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("âš¡ LLMçµ±åˆExecutorAgentãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # GitHubèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—
        github_token = get_github_token()
        
        # ExecutorAgentåˆæœŸåŒ–ï¼ˆLLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­çµ±åˆï¼‰
        executor = ExecutorAgent(
            github_token=github_token,
            repo_name="hkrhd/kaggle-claude-mother"
        )
        
        # ãƒ†ã‚¹ãƒˆç”¨å®Ÿè¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        test_request = ExecutionRequest(
            competition_name="Test Tabular Competition",
            analyzer_issue_number=12345,
            techniques_to_implement=[
                {
                    "technique": "gradient_boosting_ensemble",
                    "integrated_score": 0.85,
                    "gpu_required": False
                },
                {
                    "technique": "feature_engineering_advanced", 
                    "integrated_score": 0.72,
                    "gpu_required": False
                }
            ],
            priority=ExecutionPriority.HIGH,
            deadline_days=7
        )
        
        # å®Ÿè¡Œå®Ÿé¨“ï¼ˆLLMæå‡ºåˆ¤æ–­çµ±åˆç‰ˆï¼‰
        logger.info("ğŸš€ çµ±åˆå®Ÿè¡Œå®Ÿé¨“é–‹å§‹...")
        
        execution_result = await executor.execute_technical_implementation(test_request)
        
        logger.info("ğŸ“Š å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼:")
        logger.info(f"  å®Ÿè¡ŒID: {execution_result.execution_id}")
        logger.info(f"  ç·å®Ÿé¨“æ•°: {execution_result.total_experiments_run}")
        logger.info(f"  æˆåŠŸç‡: {execution_result.success_rate:.1%}")
        logger.info(f"  ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢: {execution_result.best_score:.6f}")
        logger.info(f"  GPUæ™‚é–“ä½¿ç”¨: {execution_result.total_gpu_hours_used:.1f}æ™‚é–“")
        logger.info(f"  å®Ÿè¡Œæ™‚é–“: {execution_result.execution_duration:.1f}ç§’")
        logger.info(f"  æå‡ºæº–å‚™: {'å®Œäº†' if execution_result.submission_ready else 'æœªå®Œ'}")
        
        # LLMæå‡ºåˆ¤æ–­çµæœç¢ºèª
        if hasattr(execution_result, 'llm_submission_decision'):
            llm_decision = execution_result.llm_submission_decision
            logger.info("ğŸ¤– LLMæå‡ºåˆ¤æ–­çµæœ:")
            logger.info(f"  åˆ¤æ–­: {llm_decision.get('decision', 'N/A')}")
            logger.info(f"  ä¿¡é ¼åº¦: {llm_decision.get('confidence', 0):.2f}")
            logger.info(f"  æ¨è«–: {llm_decision.get('reasoning', 'N/A')[:100]}...")
        
        # æå‡ºæƒ…å ±ç¢ºèª
        if hasattr(execution_result, 'submission_info'):
            submission_info = execution_result.submission_info
            logger.info("ğŸ“¤ æå‡ºæƒ…å ±:")
            logger.info(f"  æå‡ºå®Ÿè¡Œ: {submission_info.get('submitted', False)}")
            if submission_info.get('reason'):
                logger.info(f"  ç†ç”±: {submission_info['reason']}")
            if submission_info.get('llm_reasoning'):
                logger.info(f"  LLMæ¨è«–: {submission_info['llm_reasoning'][:100]}...")
        
        # ç¶™ç¶šå®Ÿé¨“æƒ…å ±ç¢ºèª
        if hasattr(execution_result, 'continued_experiments'):
            cont_exp = execution_result.continued_experiments
            logger.info("ğŸ”¬ ç¶™ç¶šå®Ÿé¨“æƒ…å ±:")
            logger.info(f"  å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {cont_exp.get('executed_actions', [])}")
            logger.info(f"  ãƒˆãƒªã‚¬ãƒ¼: {cont_exp.get('triggered_by', 'N/A')}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ çµ±åˆExecutorAgentãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_multi_agent_llm_coordination():
    """ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆLLMé€£æºãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸŒ ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆLLMé€£æºãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # GitHubèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—
        github_token = get_github_token()
        repo_name = "hkrhd/kaggle-claude-mother"
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        competition_selector = CompetitionSelectorAgent(
            github_token=github_token,
            repo_name=repo_name
        )
        
        analyzer = AnalyzerAgent()
        
        executor = ExecutorAgent(
            github_token=github_token, 
            repo_name=repo_name
        )
        
        monitor = MonitorAgent(
            github_token=github_token,
            repo_name=repo_name
        )
        
        # 1. ç«¶æŠ€é¸æŠï¼ˆæ—¢å­˜LLMãƒ™ãƒ¼ã‚¹ï¼‰
        logger.info("ğŸ¯ æ®µéš1: LLMãƒ™ãƒ¼ã‚¹ç«¶æŠ€é¸æŠ")
        
        test_competitions = [
            {
                "id": "test-tabular-competition",
                "name": "Test Tabular Competition",
                "type": "tabular",
                "deadline": (datetime.utcnow() + timedelta(days=30)).isoformat(),
                "participants": 2500,
                "prize_amount": 50000,
                "competition_category": "featured",
                "awards_medals": True
            }
        ]
        
        selection_decision = await competition_selector.evaluate_available_competitions(
            available_competitions=test_competitions,
            strategy=SelectionStrategy.BALANCED
        )
        
        logger.info(f"ç«¶æŠ€é¸æŠçµæœ: {len(selection_decision.selected_competitions)}ç«¶æŠ€é¸æŠ")
        
        if not selection_decision.selected_competitions:
            logger.warning("ç«¶æŠ€é¸æŠãªã— - ãƒ†ã‚¹ãƒˆç¶™ç¶šä¸å¯")
            return False
        
        selected_competition = selection_decision.selected_competitions[0]
        
        # 2. åˆ†æå®Ÿè¡Œï¼ˆå¾“æ¥æ–¹å¼ï¼‰
        logger.info("ğŸ”¬ æ®µéš2: æŠ€è¡“åˆ†æå®Ÿè¡Œ")
        
        analysis_request = {
            "competition_name": selected_competition.name,
            "competition_type": selected_competition.type,
            "participant_count": selected_competition.participants,
            "days_remaining": 30
        }
        
        analysis_result = await analyzer.analyze_competition(analysis_request)
        
        logger.info(f"åˆ†æå®Œäº†: {len(analysis_result.recommended_techniques)}æŠ€è¡“æ¨å¥¨")
        
        # 3. å®Ÿè¡Œå®Ÿè¡Œï¼ˆLLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­çµ±åˆï¼‰
        logger.info("âš¡ æ®µéš3: LLMçµ±åˆå®Ÿè¡Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
        
        execution_request = ExecutionRequest(
            competition_name=selected_competition.name,
            analyzer_issue_number=12345,
            techniques_to_implement=[
                {
                    "technique": tech["technique"],
                    "integrated_score": tech.get("integrated_score", 0.7),
                    "gpu_required": False
                }
                for tech in analysis_result.recommended_techniques[:2]
            ],
            priority=ExecutionPriority.HIGH,
            deadline_days=30
        )
        
        execution_result = await executor.execute_technical_implementation(execution_request)
        
        logger.info(f"å®Ÿè¡Œå®Œäº†: æˆåŠŸç‡{execution_result.success_rate:.1%}")
        
        # 4. ç›£è¦–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆå¾“æ¥æ–¹å¼ï¼‰
        logger.info("ğŸ‘ï¸ æ®µéš4: çµ±åˆç›£è¦–")
        
        # ç›£è¦–å¯¾è±¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
        monitored_agents = {
            "competition_selector": competition_selector,
            "analyzer": analyzer,
            "executor": executor
        }
        
        # çŸ­æ™‚é–“ç›£è¦–ãƒ†ã‚¹ãƒˆï¼ˆ5ç§’ï¼‰
        monitoring_issue = await monitor.start_monitoring(monitored_agents)
        
        logger.info(f"ç›£è¦–é–‹å§‹: Issue #{monitoring_issue}")
        
        # 5ç§’é–“ç›£è¦–
        await asyncio.sleep(5)
        
        await monitor.stop_monitoring()
        
        logger.info("ç›£è¦–åœæ­¢")
        
        # 5. çµ±åˆçµæœã‚µãƒãƒªãƒ¼
        logger.info("ğŸ“ˆ çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
        logger.info(f"  é¸æŠç«¶æŠ€: {selected_competition.name}")
        logger.info(f"  LLMæ¨å¥¨ã‚¹ã‚³ã‚¢: {selected_competition.llm_score:.2f}")
        logger.info(f"  åˆ†ææŠ€è¡“æ•°: {len(analysis_result.recommended_techniques)}")
        logger.info(f"  å®Ÿè¡ŒæˆåŠŸç‡: {execution_result.success_rate:.1%}")
        logger.info(f"  å®Ÿè¡Œæ™‚é–“: {execution_result.execution_duration:.1f}ç§’")
        
        # LLMåˆ¤æ–­çµ±è¨ˆ
        executor_perf = executor.submission_decision_agent.get_performance_metrics()
        selector_perf = competition_selector.get_selection_performance_metrics()
        
        logger.info("ğŸ¤– LLMåˆ¤æ–­çµ±è¨ˆ:")
        logger.info(f"  ç«¶æŠ€é¸æŠåˆ¤æ–­: {selector_perf.get('total_decisions', 0)}å›")
        logger.info(f"  æå‡ºåˆ¤æ–­: {executor_perf['total_decisions']}å›")
        logger.info(f"  ç·åˆLLMæˆåŠŸç‡: {(executor_perf['success_rate'] + selector_perf.get('success_rate', 1.0)) / 2:.1%}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé€£æºãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸ§ª LLMå¼·åŒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    test_results = []
    
    # ãƒ†ã‚¹ãƒˆ1: LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­
    print("\nğŸ¤– ãƒ†ã‚¹ãƒˆ1: LLMãƒ™ãƒ¼ã‚¹æå‡ºåˆ¤æ–­")
    print("-" * 30)
    result1 = await test_llm_submission_decision()
    test_results.append(("LLMæå‡ºåˆ¤æ–­", result1))
    
    # ãƒ†ã‚¹ãƒˆ2: LLMçµ±åˆExecutorAgent
    print("\nâš¡ ãƒ†ã‚¹ãƒˆ2: LLMçµ±åˆExecutorAgent")
    print("-" * 30)
    result2 = await test_integrated_executor_agent()
    test_results.append(("LLMçµ±åˆExecutorAgent", result2))
    
    # ãƒ†ã‚¹ãƒˆ3: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆLLMé€£æº
    print("\nğŸŒ ãƒ†ã‚¹ãƒˆ3: ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆLLMé€£æº")
    print("-" * 30)
    result3 = await test_multi_agent_llm_coordination()
    test_results.append(("ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé€£æº", result3))
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("-" * 30)
    
    passed = 0
    for test_name, result in test_results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ ãƒ†ã‚¹ãƒˆçµæœ: {passed}/{len(test_results)} PASS")
    
    if passed == len(test_results):
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ - LLMãƒ™ãƒ¼ã‚¹åˆ¤æ–­ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œç¢ºèª")
        return 0
    else:
        print("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•— - ã‚·ã‚¹ãƒ†ãƒ ç¢ºèªãŒå¿…è¦")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)