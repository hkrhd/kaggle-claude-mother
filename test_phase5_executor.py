#!/usr/bin/env python3
"""
Phase 5: å®Ÿè¡Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±åˆãƒ†ã‚¹ãƒˆ

ExecutorAgentã¨ãã®å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆå‹•ä½œã‚’æ¤œè¨¼ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã€‚
è¤‡æ•°ã‚¯ãƒ©ã‚¦ãƒ‰ç’°å¢ƒã§ã®ä¸¦åˆ—å®Ÿé¨“å®Ÿè¡Œãƒ»ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®
ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰å‹•ä½œç¢ºèªã‚’å®Ÿæ–½ã€‚
"""

import asyncio
import sys
import os
import logging
from datetime import datetime
from typing import Dict, List, Any

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'system'))

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from system.agents.executor.executor_agent import ExecutorAgent
from system.agents.executor.cloud_managers.kaggle_kernel_manager import KaggleKernelManager
from system.agents.executor.cloud_managers.colab_execution_manager import ColabExecutionManager
from system.agents.executor.cloud_managers.paperspace_manager import PaperspaceManager
from system.agents.executor.cloud_managers.resource_optimizer import CloudResourceOptimizer
from system.agents.executor.code_generators.notebook_generator import NotebookGenerator
from system.agents.executor.code_generators.experiment_designer import ExperimentDesigner
from system.agents.executor.execution_orchestrator.parallel_executor import ParallelExecutor
from system.agents.executor.optimization.hyperparameter_tuner import HyperparameterTuner

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class Phase5IntegrationTest:
    """Phase 5çµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    def __init__(self):
        self.test_results = {}
        self.error_count = 0
        
        # ãƒ†ã‚¹ãƒˆç”¨ç«¶æŠ€è¨­å®š
        self.test_competition = {
            "name": "test-tabular-competition",
            "type": "tabular",
            "dataset_size_gb": 2.5,
            "deadline_days": 7
        }
        
        # ãƒ†ã‚¹ãƒˆç”¨æŠ€è¡“ãƒªã‚¹ãƒˆ
        self.test_techniques = [
            {
                "technique": "gradient_boosting_ensemble",
                "integrated_score": 0.85,
                "estimated_runtime_hours": 2.0,
                "complexity_level": 0.7,
                "priority_score": 0.9
            },
            {
                "technique": "multi_level_stacking",
                "integrated_score": 0.78,
                "estimated_runtime_hours": 3.5,
                "complexity_level": 0.9,
                "priority_score": 0.8
            },
            {
                "technique": "neural_network",
                "integrated_score": 0.72,
                "estimated_runtime_hours": 4.0,
                "complexity_level": 0.8,
                "priority_score": 0.7
            }
        ]
    
    async def run_full_integration_test(self) -> Dict[str, Any]:
        """å®Œå…¨çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        
        logger.info("ğŸš€ Phase 5 å®Ÿè¡Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
        
        test_cases = [
            ("ExecutorAgentåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ", self.test_executor_agent_initialization),
            ("ã‚¯ãƒ©ã‚¦ãƒ‰ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ", self.test_cloud_managers_integration),
            ("ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ", self.test_code_generation_system),
            ("ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ", self.test_resource_optimization),
            ("ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ", self.test_hyperparameter_optimization),
            ("ä¸¦åˆ—å®Ÿè¡Œã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ", self.test_parallel_execution),
            ("ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æŠ€è¡“å®Ÿè£…ãƒ†ã‚¹ãƒˆ", self.test_end_to_end_execution),
            ("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ", self.test_performance_scalability)
        ]
        
        for test_name, test_func in test_cases:
            try:
                logger.info(f"ğŸ“‹ {test_name} å®Ÿè¡Œä¸­...")
                result = await test_func()
                self.test_results[test_name] = {
                    "status": "SUCCESS" if result else "FAILED",
                    "details": result if isinstance(result, dict) else {"success": result}
                }
                
                if result:
                    logger.info(f"âœ… {test_name} æˆåŠŸ")
                else:
                    logger.error(f"âŒ {test_name} å¤±æ•—")
                    self.error_count += 1
                    
            except Exception as e:
                logger.error(f"âŒ {test_name} ã‚¨ãƒ©ãƒ¼: {e}")
                self.test_results[test_name] = {
                    "status": "ERROR",
                    "error": str(e)
                }
                self.error_count += 1
        
        # ç·åˆçµæœ
        total_tests = len(test_cases)
        success_count = total_tests - self.error_count
        success_rate = success_count / total_tests
        
        summary = {
            "test_timestamp": datetime.utcnow().isoformat(),
            "total_tests": total_tests,
            "successful_tests": success_count,
            "failed_tests": self.error_count,
            "success_rate": success_rate,
            "test_results": self.test_results
        }
        
        logger.info(f"ğŸ Phase 5çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: {success_count}/{total_tests} æˆåŠŸ ({success_rate:.1%})")
        
        return summary
    
    async def test_executor_agent_initialization(self) -> Dict[str, Any]:
        """ExecutorAgentåˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        
        # ExecutorAgentä½œæˆ
        executor = ExecutorAgent(
            github_token="test_token",
            repo_name="test/repo"
        )
        
        # åŸºæœ¬å±æ€§ç¢ºèª
        assert hasattr(executor, 'logger'), "Logger not initialized"
        assert hasattr(executor, 'kaggle_manager'), "Kaggle manager not initialized"
        assert hasattr(executor, 'colab_manager'), "Colab manager not initialized"
        assert hasattr(executor, 'paperspace_manager'), "Paperspace manager not initialized"
        assert hasattr(executor, 'resource_optimizer'), "Resource optimizer not initialized"
        assert hasattr(executor, 'notebook_generator'), "Notebook generator not initialized"
        assert hasattr(executor, 'experiment_designer'), "Experiment designer not initialized"
        assert hasattr(executor, 'parallel_executor'), "Parallel executor not initialized"
        assert hasattr(executor, 'hyperparameter_tuner'), "Hyperparameter tuner not initialized"
        
        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®å‹ç¢ºèª
        assert isinstance(executor.kaggle_manager, KaggleKernelManager), "Invalid kaggle manager type"
        assert isinstance(executor.colab_manager, ColabExecutionManager), "Invalid colab manager type"
        assert isinstance(executor.paperspace_manager, PaperspaceManager), "Invalid paperspace manager type"
        assert isinstance(executor.resource_optimizer, CloudResourceOptimizer), "Invalid resource optimizer type"
        assert isinstance(executor.notebook_generator, NotebookGenerator), "Invalid notebook generator type"
        assert isinstance(executor.experiment_designer, ExperimentDesigner), "Invalid experiment designer type"
        assert isinstance(executor.parallel_executor, ParallelExecutor), "Invalid parallel executor type"
        assert isinstance(executor.hyperparameter_tuner, HyperparameterTuner), "Invalid hyperparameter tuner type"
        
        return {
            "executor_initialized": True,
            "components_count": 8,
            "all_components_valid": True
        }
    
    async def test_cloud_managers_integration(self) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¦ãƒ‰ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ"""
        
        # å„ã‚¯ãƒ©ã‚¦ãƒ‰ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–ãƒ»åŸºæœ¬å‹•ä½œç¢ºèª
        kaggle_manager = KaggleKernelManager()
        colab_manager = ColabExecutionManager()
        paperspace_manager = PaperspaceManager()
        
        # ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ…‹å–å¾—ãƒ†ã‚¹ãƒˆ
        kaggle_status = await kaggle_manager.get_resource_status()
        colab_status = await colab_manager.get_resource_status()
        paperspace_status = await paperspace_manager.get_resource_status()
        
        assert "resource_usage" in kaggle_status, "Kaggle resource status invalid"
        assert "resource_usage" in colab_status, "Colab resource status invalid"
        assert "resource_usage" in paperspace_status, "Paperspace resource status invalid"
        
        # ã‚³ã‚¹ãƒˆæ¨å®šãƒ†ã‚¹ãƒˆ
        kaggle_cost = await kaggle_manager.estimate_execution_cost(0.7, "tabular")
        colab_cost = await colab_manager.estimate_execution_cost(0.7, "tabular")
        paperspace_cost = await paperspace_manager.estimate_execution_cost(0.7, "tabular")
        
        assert "estimated_gpu_hours" in kaggle_cost, "Kaggle cost estimation invalid"
        assert "estimated_gpu_hours" in colab_cost, "Colab cost estimation invalid"
        assert "estimated_gpu_hours" in paperspace_cost, "Paperspace cost estimation invalid"
        
        return {
            "kaggle_manager_working": True,
            "colab_manager_working": True,
            "paperspace_manager_working": True,
            "resource_status_available": True,
            "cost_estimation_available": True
        }
    
    async def test_code_generation_system(self) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        
        notebook_generator = NotebookGenerator()
        experiment_designer = ExperimentDesigner()
        
        # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç”Ÿæˆãƒ†ã‚¹ãƒˆ
        test_technique = self.test_techniques[0]
        notebooks = await notebook_generator.generate_technique_notebooks(
            technique_name=test_technique["technique"],
            competition_type=self.test_competition["type"],
            dataset_info={"size_gb": 2.5, "features": 100},
            resource_constraints={"max_gpu_hours": 4.0}
        )
        
        assert isinstance(notebooks, dict), "Notebooks should be dictionary"
        assert len(notebooks) > 0, "No notebooks generated"
        
        # å®Ÿé¨“è¨­è¨ˆãƒ†ã‚¹ãƒˆ
        experiment_plan = await experiment_designer.design_experiments(
            techniques=self.test_techniques[:2],
            notebooks={},
            resource_constraints={"max_gpu_hours": 8.0}
        )
        
        assert hasattr(experiment_plan, 'experiment_id'), "Experiment plan missing ID"
        assert hasattr(experiment_plan, 'configs'), "Experiment plan missing configs"
        assert len(experiment_plan.configs) > 0, "No experiment configs generated"
        
        return {
            "notebook_generation_working": True,
            "generated_notebooks": len(notebooks),
            "experiment_design_working": True,
            "experiment_configs": len(experiment_plan.configs),
            "estimated_total_time": experiment_plan.estimated_total_time
        }
    
    async def test_resource_optimization(self) -> Dict[str, Any]:
        """ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        
        # ResourceOptimizeråˆæœŸåŒ–
        optimizer = CloudResourceOptimizer()
        
        # ç¾åœ¨ã®ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ…‹å–å¾—
        current_resources = await optimizer.get_current_resource_state()
        
        assert isinstance(current_resources, dict), "Resource state should be dictionary"
        
        # å®Ÿé¨“è¦ä»¶ä½œæˆ
        from system.agents.executor.cloud_managers.resource_optimizer import ExecutionRequirement
        
        requirements = []
        for technique in self.test_techniques[:2]:
            req = ExecutionRequirement(
                technique_name=technique["technique"],
                competition_name=self.test_competition["name"],
                estimated_gpu_hours=technique["estimated_runtime_hours"],
                estimated_cpu_hours=technique["estimated_runtime_hours"] * 0.2,
                memory_gb_required=8.0,
                storage_gb_required=5.0,
                deadline_hours=168,  # 1é€±é–“
                priority_score=technique["priority_score"],
                complexity_level=technique["complexity_level"]
            )
            requirements.append(req)
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        allocations = await optimizer.optimize_experiment_allocation(requirements)
        
        assert isinstance(allocations, list), "Allocations should be list"
        assert len(allocations) > 0, "No allocations generated"
        
        # æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆå–å¾—
        report = await optimizer.get_optimization_report()
        
        assert "platform_efficiency" in report, "Optimization report missing platform efficiency"
        
        return {
            "resource_optimization_working": True,
            "allocations_generated": len(allocations),
            "platforms_analyzed": len(current_resources),
            "optimization_report_available": True
        }
    
    async def test_hyperparameter_optimization(self) -> Dict[str, Any]:
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
        
        tuner = HyperparameterTuner()
        
        # æœ€é©åŒ–è¨­å®šä½œæˆ
        config = await tuner.create_technique_optimization_config(
            technique_name="gradient_boosting_ensemble",
            competition_type="tabular",
            resource_constraints={"max_gpu_hours": 2.0}
        )
        
        assert hasattr(config, 'study_name'), "Config missing study name"
        assert hasattr(config, 'parameter_specs'), "Config missing parameter specs"
        assert len(config.parameter_specs) > 0, "No parameter specs defined"
        
        # ç°¡æ˜“ç›®çš„é–¢æ•°å®šç¾©
        def mock_objective(params: Dict[str, Any]) -> float:
            # æ¨¡æ“¬ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢
            return 0.85 + (hash(str(params)) % 1000) / 10000
        
        # æœ€é©åŒ–å®Ÿè¡Œï¼ˆçŸ­æ™‚é–“ã§ï¼‰
        config.max_trials = 5
        config.max_time_hours = 0.1  # 6åˆ†
        
        result = await tuner.optimize_hyperparameters(
            config=config,
            objective_function=mock_objective
        )
        
        assert result.success, "Optimization should succeed"
        assert result.n_trials > 0, "No trials executed"
        assert len(result.best_params) > 0, "No best params found"
        
        # æœ€é©åŒ–ã‚µãƒãƒªãƒ¼å–å¾—
        summary = await tuner.get_optimization_summary()
        
        assert "total_optimizations" in summary, "Summary missing total optimizations"
        
        return {
            "hyperparameter_optimization_working": True,
            "trials_executed": result.n_trials,
            "best_score": result.best_score,
            "parameter_count": len(result.best_params),
            "optimization_time_hours": result.optimization_time_hours
        }
    
    async def test_parallel_execution(self) -> Dict[str, Any]:
        """ä¸¦åˆ—å®Ÿè¡Œã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
        
        # ParallelExecutoråˆæœŸåŒ–
        parallel_executor = ParallelExecutor()
        
        # æ¨¡æ“¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰
        mock_notebooks = {
            "gradient_boosting_ensemble": {
                "kaggle_kernels": "# Mock Kaggle notebook for GBM",
                "google_colab": "# Mock Colab notebook for GBM"
            },
            "multi_level_stacking": {
                "kaggle_kernels": "# Mock Kaggle notebook for Stacking",
                "paperspace_gradient": "# Mock Paperspace notebook for Stacking"
            }
        }
        
        # ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆï¼ˆçŸ­æ™‚é–“ã§ï¼‰
        execution_settings = {
            "competition_name": self.test_competition["name"],
            "max_retries": 1
        }
        
        # å®Ÿè¡Œæ™‚é–“ã‚’çŸ­ç¸®
        for technique in self.test_techniques[:2]:
            technique["estimated_runtime_hours"] = 0.1  # 6åˆ†ã«çŸ­ç¸®
        
        parallel_execution = await parallel_executor.execute_parallel_techniques(
            technique_configs=self.test_techniques[:2],
            notebook_codes=mock_notebooks,
            execution_settings=execution_settings
        )
        
        assert hasattr(parallel_execution, 'execution_id'), "Parallel execution missing ID"
        assert hasattr(parallel_execution, 'results'), "Parallel execution missing results"
        assert parallel_execution.total_tasks > 0, "No tasks created"
        
        # å®Ÿè¡ŒçŠ¶æ…‹å–å¾—
        status = await parallel_executor.get_execution_status(parallel_execution.execution_id)
        
        assert status is not None, "Execution status not available"
        assert "status" in status, "Status missing status field"
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼å–å¾—
        performance = await parallel_executor.get_performance_summary()
        
        assert "recent_executions" in performance, "Performance summary missing recent executions"
        
        return {
            "parallel_execution_working": True,
            "tasks_created": parallel_execution.total_tasks,
            "execution_id": parallel_execution.execution_id,
            "results_count": len(parallel_execution.results),
            "success_rate": parallel_execution.success_rate
        }
    
    async def test_end_to_end_execution(self) -> Dict[str, Any]:
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æŠ€è¡“å®Ÿè£…ãƒ†ã‚¹ãƒˆ"""
        
        # ExecutorAgentä½œæˆ
        executor = ExecutorAgent(
            github_token="test_token",
            repo_name="test/repo"
        )
        
        # æŠ€è¡“å®Ÿè£…å®Ÿè¡Œï¼ˆãƒ•ãƒ«çµ±åˆï¼‰
        technique_info = self.test_techniques[0].copy()
        technique_info["estimated_runtime_hours"] = 0.2  # 12åˆ†ã«çŸ­ç¸®
        
        # ExecutionRequestã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
        from system.agents.executor.executor_agent import ExecutionRequest, ExecutionPriority
        
        execution_request = ExecutionRequest(
            competition_name=self.test_competition["name"],
            analyzer_issue_number=123,
            techniques_to_implement=[technique_info],
            priority=ExecutionPriority.HIGH,
            deadline_days=7,
            resource_constraints={
                "max_gpu_hours": 1.0,
                "priority": "high"
            }
        )
        
        result = await executor.execute_technical_implementation(
            request=execution_request
        )
        
        assert result is not None, "Implementation result is None"
        assert hasattr(result, "execution_id"), "Result missing execution_id"
        assert hasattr(result, "kaggle_results"), "Result missing kaggle_results"
        
        return {
            "end_to_end_execution_working": True,
            "technique_implemented": technique_info["technique"],
            "execution_id": result.execution_id,
            "kaggle_results_count": len(result.kaggle_results),
            "colab_results_count": len(result.colab_results),
            "paperspace_results_count": len(result.paperspace_results),
            "total_gpu_hours_used": result.total_gpu_hours_used
        }
    
    async def test_performance_scalability(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        
        # ExecutorAgentä½œæˆ
        executor = ExecutorAgent(
            github_token="test_token",
            repo_name="test/repo"
        )
        
        # è¤‡æ•°æŠ€è¡“ã®å®Ÿè¡Œãƒ†ã‚¹ãƒˆï¼ˆç°¡ç•¥åŒ–ï¼‰
        multiple_techniques = self.test_techniques[:2].copy()
        for technique in multiple_techniques:
            technique["estimated_runtime_hours"] = 0.1  # 6åˆ†ã«çŸ­ç¸®
        
        # è¤‡æ•°ã®ExecutionRequestã‚’ä½œæˆ
        from system.agents.executor.executor_agent import ExecutionRequest, ExecutionPriority
        
        requests = []
        for technique in multiple_techniques:
            request = ExecutionRequest(
                competition_name=self.test_competition["name"],
                analyzer_issue_number=124,
                techniques_to_implement=[technique],
                priority=ExecutionPriority.STANDARD,
                deadline_days=7,
                resource_constraints={"max_gpu_hours": 1.0}
            )
            requests.append(request)
        
        # å„æŠ€è¡“ã‚’é †ç•ªã«å®Ÿè¡Œï¼ˆä¸¦åˆ—å®Ÿè¡Œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        results = []
        for request in requests:
            try:
                result = await executor.execute_technical_implementation(request=request)
                results.append(result)
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦ãƒ†ã‚¹ãƒˆã‚’ç¶šè¡Œ
                results.append({"error": str(e)})
        
        # åŸºæœ¬çš„ãªæ¤œè¨¼
        assert len(results) == len(multiple_techniques), "Not all techniques executed"
        
        return {
            "performance_scalability_working": True,
            "batch_techniques_count": len(multiple_techniques),
            "results_count": len(results),
            "execution_success": len([r for r in results if "error" not in r]),
            "error_count": len([r for r in results if "error" in r])
        }


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸ¯ Phase 5: ExecutorAgentçµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_suite = Phase5IntegrationTest()
    results = await test_suite.run_full_integration_test()
    
    # çµæœè¡¨ç¤º
    print("\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
    print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {results['total_tests']}")
    print(f"æˆåŠŸ: {results['successful_tests']}")
    print(f"å¤±æ•—: {results['failed_tests']}")
    print(f"æˆåŠŸç‡: {results['success_rate']:.1%}")
    
    print("\nğŸ“‹ è©³ç´°çµæœ:")
    for test_name, result in results['test_results'].items():
        status_emoji = "âœ…" if result['status'] == 'SUCCESS' else "âŒ"
        print(f"{status_emoji} {test_name}: {result['status']}")
        
        if result['status'] == 'ERROR':
            print(f"   ã‚¨ãƒ©ãƒ¼: {result.get('error', 'Unknown error')}")
        elif 'details' in result and isinstance(result['details'], dict):
            for key, value in result['details'].items():
                if isinstance(value, (int, float, bool, str)):
                    print(f"   {key}: {value}")
    
    print("\n" + "=" * 60)
    
    if results['success_rate'] >= 0.8:
        print("ğŸ‰ Phase 5çµ±åˆãƒ†ã‚¹ãƒˆ æˆåŠŸï¼")
        print("ExecutorAgentã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        return 0
    else:
        print("âš ï¸  Phase 5çµ±åˆãƒ†ã‚¹ãƒˆ éƒ¨åˆ†çš„æˆåŠŸ")
        print("ä¸€éƒ¨ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)