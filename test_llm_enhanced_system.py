#!/usr/bin/env python3
"""
LLMå¼·åŒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ»å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®LLMçµ±åˆãƒ»ãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–ã®
ç·åˆçš„ãªå‹•ä½œç¢ºèªãƒ»æ€§èƒ½ãƒ†ã‚¹ãƒˆã€‚
"""

import asyncio
import sys
import os
import logging
import subprocess
from datetime import datetime, timedelta

# ãƒ‘ã‚¹è¨­å®š
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'system'))

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
from system.prompts.prompt_manager import PromptManager, PromptType

# LLMçµ±åˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
from system.agents.analyzer.analyzer_agent import AnalyzerAgent, AnalysisRequest, AnalysisScope
from system.agents.monitor.monitor_agent import MonitorAgent, SystemAlert, AlertSeverity
from system.agents.executor.executor_agent import ExecutorAgent, ExecutionRequest, ExecutionPriority

# æ—¢å­˜ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
from system.agents.executor.submission_decision_agent import SubmissionDecisionAgent, SubmissionContext
from system.agents.shared.llm_decision_base import ClaudeClient


def get_github_token():
    """GitHubèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‹•çš„å–å¾—"""
    try:
        result = subprocess.run(['gh', 'auth', 'token'], 
                              capture_output=True, text=True, check=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        raise Exception(f"GitHubèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—å¤±æ•—: {e}")


async def test_prompt_management_system():
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ”§ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        prompt_manager = PromptManager()
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        prompt_manager.create_prompt_directory()
        
        # å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—ã®ãƒ†ã‚¹ãƒˆ
        test_contexts = {
            PromptType.TECHNICAL_ANALYSIS: {
                "competition_name": "Test ML Competition",
                "competition_type": "tabular",
                "total_teams": 2500,
                "days_remaining": 10,
                "medal_target": "gold"
            },
            PromptType.ANOMALY_DIAGNOSIS: {
                "detection_timestamp": datetime.utcnow().isoformat(),
                "affected_systems": ["executor"],
                "error_messages": ["å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ"],
                "urgency_level": "high"
            }
        }
        
        success_count = 0
        for prompt_type, context in test_contexts.items():
            try:
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
                rendered_prompt = prompt_manager.get_optimized_prompt(
                    prompt_type=prompt_type,
                    context_data=context
                )
                
                if rendered_prompt and len(rendered_prompt) > 100:
                    success_count += 1
                    logger.info(f"  âœ… {prompt_type.value}: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”ŸæˆæˆåŠŸ")
                else:
                    logger.warning(f"  âš ï¸ {prompt_type.value}: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆä¸ååˆ†")
                
            except Exception as e:
                logger.error(f"  âŒ {prompt_type.value}: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå¤±æ•— - {e}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ±è¨ˆç¢ºèª
        stats = prompt_manager.get_prompt_stats()
        logger.info(f"  ğŸ“Š ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ±è¨ˆ: {stats}")
        
        test_success = success_count >= len(test_contexts) // 2
        logger.info(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ : {'âœ… PASS' if test_success else 'âŒ FAIL'}")
        return test_success
        
    except Exception as e:
        logger.error(f"âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False


async def test_analyzer_llm_integration():
    """AnalyzerAgent LLMçµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ§  AnalyzerAgent LLMçµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # AnalyzerAgentåˆæœŸåŒ–ï¼ˆLLMçµ±åˆç‰ˆï¼‰
        analyzer = AnalyzerAgent()
        
        # LLMçµ±åˆãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        if not analyzer.llm_enabled:
            logger.warning("LLMçµ±åˆãŒç„¡åŠ¹ - æœ‰åŠ¹åŒ–ã—ã¾ã™")
            analyzer.enable_llm_integration(True)
        
        # ãƒ†ã‚¹ãƒˆç”¨åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        test_request = AnalysisRequest(
            competition_name="LLM Integration Test Competition",
            competition_type="tabular",
            participant_count=3000,
            days_remaining=14,
            scope=AnalysisScope.QUICK
        )
        
        # åˆ†æå®Ÿè¡Œï¼ˆLLMçµ±åˆç‰ˆï¼‰
        logger.info("ğŸš€ LLMçµ±åˆåˆ†æå®Ÿè¡Œ...")
        
        # å®Ÿéš›ã®åˆ†æå®Ÿè¡Œï¼ˆæ™‚é–“çŸ­ç¸®ã®ãŸã‚æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã§ï¼‰
        # å®Ÿç’°å¢ƒã§ã¯: analysis_result = await analyzer.analyze_competition(test_request)
        
        # æ¨¡æ“¬åˆ†æçµæœã§çµ±åˆã‚·ã‚¹ãƒ†ãƒ å‹•ä½œç¢ºèª
        mock_result = await analyzer._create_mock_analysis_result(test_request)
        
        # LLMçµ±åˆçµ±è¨ˆç¢ºèª
        llm_stats = analyzer.get_llm_integration_stats()
        logger.info(f"  ğŸ“Š LLMçµ±åˆçµ±è¨ˆ: {llm_stats}")
        
        # åˆ†æçµæœå“è³ªãƒã‚§ãƒƒã‚¯
        if hasattr(mock_result, 'recommended_techniques') and len(mock_result.recommended_techniques) > 0:
            logger.info(f"  âœ… æŠ€è¡“æ¨å¥¨ç”Ÿæˆ: {len(mock_result.recommended_techniques)}æŠ€è¡“")
            logger.info(f"  ğŸ“ˆ ä¿¡é ¼åº¦: {mock_result.confidence_level:.2f}")
            
            if hasattr(mock_result, 'llm_integration_result'):
                logger.info("  ğŸ¤– LLMçµ±åˆçµæœ: å–å¾—æˆåŠŸ")
            
            return True
        else:
            logger.warning("  âš ï¸ æŠ€è¡“æ¨å¥¨ç”Ÿæˆä¸è¶³")
            return False
        
    except Exception as e:
        logger.error(f"âŒ AnalyzerAgent LLMçµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_monitor_llm_diagnosis():
    """MonitorAgent LLMè¨ºæ–­ãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ‘ï¸ MonitorAgent LLMè¨ºæ–­ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # MonitorAgentåˆæœŸåŒ–ï¼ˆLLMçµ±åˆç‰ˆï¼‰
        github_token = get_github_token()
        monitor = MonitorAgent(
            github_token=github_token,
            repo_name="hkrhd/kaggle-claude-mother"
        )
        
        # LLMè¨ºæ–­ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        if not monitor.llm_enabled:
            logger.warning("LLMè¨ºæ–­ãŒç„¡åŠ¹ - æœ‰åŠ¹åŒ–ã—ã¾ã™")
            monitor.enable_llm_diagnosis(True)
        
        # ãƒ†ã‚¹ãƒˆç”¨ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆä½œæˆ
        test_alert = SystemAlert(
            alert_id="test-alert-001",
            timestamp=datetime.utcnow(),
            severity=AlertSeverity.WARNING,
            source_agent="executor",
            title="ğŸ§ª ãƒ†ã‚¹ãƒˆç”¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹ã‚¢ãƒ©ãƒ¼ãƒˆ", 
            description="æˆåŠŸç‡: 65%, CPU: 85%, ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡é«˜",
            affected_components=["executor-agent"],
            metrics_snapshot={"success_rate": 0.65, "cpu_usage": 85.0}
        )
        
        # æ¨¡æ“¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
        from system.agents.monitor.monitor_agent import PerformanceMetrics
        test_metrics = [
            PerformanceMetrics(
                timestamp=datetime.utcnow(),
                agent_type="executor",
                tasks_completed=10,
                tasks_failed=3,
                success_rate=0.65,
                cpu_usage_percent=85.0,
                memory_usage_mb=6500,
                gpu_usage_percent=0.0,
                gpu_memory_mb=0.0,
                api_calls_count=150,
                api_rate_limit_remaining=250
            )
        ]
        
        # LLMãƒ™ãƒ¼ã‚¹ç•°å¸¸è¨ºæ–­å®Ÿè¡Œ
        logger.info("ğŸ” LLMç•°å¸¸è¨ºæ–­å®Ÿè¡Œ...")
        
        await monitor._perform_llm_anomaly_diagnosis(test_alert, test_metrics)
        
        # è¨ºæ–­çµæœç¢ºèª
        if hasattr(test_alert, 'llm_diagnosis') and test_alert.llm_diagnosis:
            diagnosis = test_alert.llm_diagnosis
            logger.info("  âœ… LLMè¨ºæ–­å®Œäº†")
            logger.info(f"  ğŸ” ä¸»åŸå› : {diagnosis.get('diagnosis_summary', {}).get('primary_cause', 'N/A')}")
            logger.info(f"  âš¡ ä¿¡é ¼åº¦: {diagnosis.get('diagnosis_summary', {}).get('confidence_level', 0):.2f}")
            logger.info(f"  ğŸš€ å³æ™‚å¯¾å¿œæ•°: {len(diagnosis.get('immediate_actions', []))}")
            
            # LLMè¨ºæ–­çµ±è¨ˆç¢ºèª
            diagnosis_stats = monitor.get_llm_diagnosis_stats()
            logger.info(f"  ğŸ“Š LLMè¨ºæ–­çµ±è¨ˆ: {diagnosis_stats}")
            
            return True
        else:
            logger.warning("  âš ï¸ LLMè¨ºæ–­çµæœæœªå–å¾—")
            return False
        
    except Exception as e:
        logger.error(f"âŒ MonitorAgent LLMè¨ºæ–­ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_integrated_medal_optimization():
    """çµ±åˆãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
    
    logger = logging.getLogger(__name__)
    logger.info("ğŸ† çµ±åˆãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    try:
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆæœŸåŒ–
        claude_client = ClaudeClient()
        submission_agent = SubmissionDecisionAgent(claude_client)
        
        # ãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª
        medal_scenarios = [
            {
                "name": "Goldç²å¾—å¯èƒ½ã‚·ãƒŠãƒªã‚ª",
                "context": SubmissionContext(
                    competition_name="Gold Target Competition",
                    current_best_score=0.9234,
                    target_score=0.9100,
                    current_rank_estimate=15,
                    total_participants=3500,
                    days_remaining=3,
                    hours_remaining=72.0,
                    experiments_completed=45,
                    experiments_running=2,
                    success_rate=0.88,
                    resource_budget_remaining=0.25,
                    score_history=[0.8987, 0.9045, 0.9123, 0.9189, 0.9234],
                    score_improvement_trend=0.0062,
                    plateau_duration_hours=2.5,
                    leaderboard_top10_scores=[
                        0.9456, 0.9398, 0.9345, 0.9298, 0.9267,
                        0.9245, 0.9234, 0.9201, 0.9178, 0.9156
                    ],
                    medal_threshold_estimate=0.9200,
                    current_medal_zone="silver",
                    model_stability=0.92,
                    overfitting_risk=0.15,
                    technical_debt_level=0.20
                ),
                "expected_decision": "SUBMIT"
            },
            {
                "name": "æ”¹å–„ç¶™ç¶šã‚·ãƒŠãƒªã‚ª",
                "context": SubmissionContext(
                    competition_name="Improvement Focus Competition",
                    current_best_score=0.8567,
                    target_score=0.9000,
                    current_rank_estimate=450,
                    total_participants=2500,
                    days_remaining=10,
                    hours_remaining=240.0,
                    experiments_completed=25,
                    experiments_running=3,
                    success_rate=0.76,
                    resource_budget_remaining=0.65,
                    score_history=[0.8234, 0.8345, 0.8456, 0.8512, 0.8567],
                    score_improvement_trend=0.0083,
                    plateau_duration_hours=1.2,
                    leaderboard_top10_scores=[
                        0.9234, 0.9156, 0.9089, 0.9012, 0.8967,
                        0.8934, 0.8901, 0.8878, 0.8856, 0.8834
                    ],
                    medal_threshold_estimate=0.8800,
                    current_medal_zone="none",
                    model_stability=0.85,
                    overfitting_risk=0.25,
                    technical_debt_level=0.30
                ),
                "expected_decision": "CONTINUE"
            }
        ]
        
        success_count = 0
        for scenario in medal_scenarios:
            logger.info(f"  ğŸ¯ ã‚·ãƒŠãƒªã‚ª: {scenario['name']}")
            
            # ãƒ¡ãƒ€ãƒ«æœ€é©åŒ–åˆ¤æ–­å®Ÿè¡Œ
            decision_response = await submission_agent.should_submit_competition(
                context=scenario["context"],
                urgency="medium"
            )
            
            decision = decision_response.decision_result["decision"]
            confidence = decision_response.confidence_score
            medal_prob = decision_response.decision_result.get("medal_probability", {})
            
            logger.info(f"    åˆ¤æ–­: {decision} (ä¿¡é ¼åº¦: {confidence:.2f})")
            logger.info(f"    Goldç¢ºç‡: {medal_prob.get('gold', 0):.2f}, Silverç¢ºç‡: {medal_prob.get('silver', 0):.2f}")
            
            # æœŸå¾…åˆ¤æ–­ã¨ã®æ¯”è¼ƒ
            if decision == scenario["expected_decision"]:
                logger.info("    âœ… æœŸå¾…ã•ã‚Œã‚‹åˆ¤æ–­çµæœ")
                success_count += 1
            else:
                logger.warning(f"    âš ï¸ äºˆæƒ³å¤–ã®åˆ¤æ–­: æœŸå¾…={scenario['expected_decision']}, å®Ÿéš›={decision}")
        
        # ç·åˆãƒ¡ãƒ€ãƒ«æœ€é©åŒ–è©•ä¾¡
        optimization_success = success_count >= len(medal_scenarios) // 2
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçµ±è¨ˆã‚µãƒãƒªãƒ¼
        submission_stats = submission_agent.get_performance_metrics()
        logger.info(f"  ğŸ“Š æå‡ºåˆ¤æ–­çµ±è¨ˆ: {submission_stats}")
        
        logger.info(f"çµ±åˆãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–: {'âœ… PASS' if optimization_success else 'âŒ FAIL'}")
        return optimization_success
        
    except Exception as e:
        logger.error(f"âŒ çµ±åˆãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸ§ª LLMå¼·åŒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    test_results = []
    
    # ãƒ†ã‚¹ãƒˆ1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
    print("\nğŸ”§ ãƒ†ã‚¹ãƒˆ1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
    print("-" * 30)
    result1 = await test_prompt_management_system()
    test_results.append(("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ", result1))
    
    # ãƒ†ã‚¹ãƒˆ2: AnalyzerAgent LLMçµ±åˆ
    print("\nğŸ§  ãƒ†ã‚¹ãƒˆ2: AnalyzerAgent LLMçµ±åˆ")
    print("-" * 30)
    result2 = await test_analyzer_llm_integration()
    test_results.append(("AnalyzerAgent LLMçµ±åˆ", result2))
    
    # ãƒ†ã‚¹ãƒˆ3: MonitorAgent LLMè¨ºæ–­
    print("\nğŸ‘ï¸ ãƒ†ã‚¹ãƒˆ3: MonitorAgent LLMè¨ºæ–­")
    print("-" * 30)
    result3 = await test_monitor_llm_diagnosis()
    test_results.append(("MonitorAgent LLMè¨ºæ–­", result3))
    
    # ãƒ†ã‚¹ãƒˆ4: çµ±åˆãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–
    print("\nğŸ† ãƒ†ã‚¹ãƒˆ4: çµ±åˆãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–")
    print("-" * 30)
    result4 = await test_integrated_medal_optimization()
    test_results.append(("çµ±åˆãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–", result4))
    
    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("ğŸ“Š çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("-" * 30)
    
    passed = 0
    for test_name, result in test_results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ ãƒ†ã‚¹ãƒˆçµæœ: {passed}/{len(test_results)} PASS")
    
    if passed == len(test_results):
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ - LLMå¼·åŒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å‹•ä½œç¢ºèª")
        print("\nğŸ† ãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ å®Œæˆ:")
        print("  âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  (.mdãƒ•ã‚¡ã‚¤ãƒ«åˆ†é›¢)")
        print("  âœ… AnalyzerAgent LLMæŠ€è¡“åˆ†æçµ±åˆ")
        print("  âœ… MonitorAgent LLMç•°å¸¸è¨ºæ–­çµ±åˆ")
        print("  âœ… çµ±åˆãƒ¡ãƒ€ãƒ«ç²å¾—æœ€é©åŒ–")
        return 0
    else:
        print("âš ï¸ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•— - ã‚·ã‚¹ãƒ†ãƒ ç¢ºèªãŒå¿…è¦")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)